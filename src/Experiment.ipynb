{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Environments import (CartPole, Highway, Hopper, LunarLander,\n",
    "                          Swimmer)\n",
    "from LLM.LLMOptions import llm_options\n",
    "from log.log_config import init_logger\n",
    "from VIRAL import VIRAL\n",
    "init_logger(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runs(\n",
    "    total_timesteps: int,\n",
    "    nb_vec_envs: int,\n",
    "    nb_refined: int,\n",
    "    human_feedback: bool,\n",
    "    video_description: bool,\n",
    "    legacy_training: bool,\n",
    "    actor_model: str,\n",
    "    critic_model: str,\n",
    "    env: str,\n",
    "    observation_space: str,\n",
    "    goal: str,\n",
    "    image: str,\n",
    "    nb_gen: int,\n",
    "    nb_runs: int,\n",
    "    proxies: dict,\n",
    "    focus: str = \"\",\n",
    "):\n",
    "    \"\"\"help wrapper for launch several runs\n",
    "\n",
    "    Args:\n",
    "        total_timesteps (int): \n",
    "        nb_vec_envs (int): \n",
    "        nb_refined (int): \n",
    "        human_feedback (bool): \n",
    "        video_description (bool): \n",
    "        legacy_training (bool): \n",
    "        actor_model (str): \n",
    "        critic_model (str): \n",
    "        env (str): \n",
    "        observation_space (str): \n",
    "        goal (str): \n",
    "        image (str): \n",
    "        nb_gen (int): \n",
    "        nb_runs (int): \n",
    "        proxies (dict): \n",
    "        focus (str, optional): . Defaults to \"\".\n",
    "    \"\"\"\n",
    "    switcher = {\n",
    "        \"Cartpole\": CartPole,\n",
    "        \"LunarLander\": LunarLander,\n",
    "        \"Highway\": Highway,\n",
    "        \"Swimmer\": Swimmer,\n",
    "        \"Hopper\": Hopper,\n",
    "    }\n",
    "    instance = switcher[env]()\n",
    "    if observation_space != \"\":\n",
    "        instance.prompt[\"Observation Space\"] = observation_space\n",
    "    if goal is not None:\n",
    "        instance.prompt[\"Goal\"] = goal\n",
    "    else:\n",
    "        instance.prompt.pop(\"Goal\", None)\n",
    "    if image is not None:\n",
    "        instance.prompt[\"Image\"] = image\n",
    "    else:\n",
    "        instance.prompt.pop(\"Image\", None)\n",
    "    def run():\n",
    "        viral = VIRAL(\n",
    "            env_type=instance,\n",
    "            model_actor=actor_model,\n",
    "            model_critic=critic_model,\n",
    "            hf=human_feedback,\n",
    "            vd=video_description,\n",
    "            nb_vec_envs=nb_vec_envs,\n",
    "            options=llm_options,\n",
    "            legacy_training=legacy_training,\n",
    "            training_time=total_timesteps,\n",
    "            proxies=proxies,\n",
    "        )\n",
    "        viral.generate_context()\n",
    "        viral.generate_reward_function(nb_gen, nb_refined, focus)\n",
    "        viral.policy_trainer.start_vd(viral.memory[1].policy, 1)\n",
    "\n",
    "    for r in range(nb_runs):\n",
    "        print(f\"#######  {r}  ########\")\n",
    "        run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies = { \n",
    "\t\"http\"  : \"socks5h://localhost:1080\", \n",
    "\t\"https\" : \"socks5h://localhost:1080\", \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_space = \"\"\"Box([ -2.5 -2.5 -10. -10. -6.2831855 -10. -0. -0. ], \n",
    "[ 2.5 2.5 10. 10. 6.2831855 10. 1. 1. ], (8,), float32)\n",
    "The state is an 8-dimensional vector: \n",
    "the coordinates of the lander in x & y, \n",
    "its linear velocities in x & y, \n",
    "its angle, its angular velocity, \n",
    "and two booleans that represent whether each leg is in contact with the ground or not.\n",
    "\"\"\"\n",
    "goal = \"Do not land but do not crash, i want a stationary Flight\"\n",
    "image = 'Environments/img/stationary_flight.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:09:21 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 237834}\n",
      "\n",
      "22:09:21 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 237834}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  0  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:09:27 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HELP>\n",
      "The given observation space describes an 8-dimensional vector representing the state of the lander in a 2D environment. The vector is composed of:\n",
      "\n",
      "* `x` and `y` coordinates of the lander: These are the positions of the lander on the x and y axes, ranging from `-2.5` to `2.5`.\n",
      "\n",
      "* `vx` and `vy`: These represent the linear velocities of the lander in the x and y directions, respectively.\n",
      "\n",
      "* `theta`: This is the angle of the lander with respect to the horizontal plane, which can range from `-6.2831855` (π radians) to `10`. Note that this range seems inconsistent; typically, angles would be defined within a single period of 2π or π radians for simplicity.\n",
      "\n",
      "* `omega`: This represents the angular velocity of the lander around its vertical axis.\n",
      "\n",
      "* `leg1_contact` and `leg2_contact`: These are two boolean values indicating whether each leg is in contact with the ground or not.\n",
      "\n",
      "To achieve a stationary flight, meaning to hover without crashing but also preventing landing (i.e., maintaining altitude), the agent must balance three main aspects:\n",
      "\n",
      "1. **Vertical Velocity Control**: The agent needs to control the vertical velocity (`vy`) to zero to prevent descending and maintain the current altitude. This involves regulating `vy` within a small range around zero.\n",
      "\n",
      "2. **Horizontal Positioning**: For stationary flight, the horizontal movement should be minimized or ideally controlled at a constant speed to maintain position over time. This involves keeping `vx` close to zero but with some tolerance depending on the desired hovering stability.\n",
      "\n",
      "3. **Altitude Maintenance (Preventing Landing)**: The agent must avoid letting the lander descend further than its current altitude by controlling `theta`, ensuring that `vy` does not become negative (indicating descent). This involves keeping the vertical velocity component of the movement in check, often done through fine adjustments to `theta`.\n",
      "\n",
      "4. **Angular Velocity and Orientation**: For a stationary flight without drifting away from the desired position or orientation, managing `omega` is crucial. The agent must stabilize the lander's angular velocity around its vertical axis to prevent unwanted turns.\n",
      "\n",
      "5. **Leg Contact Avoidance (Preventing Landing)**: The agent should ensure that at least one leg remains in contact with the ground (`leg1_contact` and/or `leg2_contact`) but avoid conditions where both legs are on the ground, which would cause landing. This involves understanding the dynamics of each leg's interaction with the ground.\n",
      "\n",
      "To achieve these goals, the reinforcement learning algorithm can utilize a reward function that penalizes large deviations in any dimension (especially altitude, vertical velocity, and angular velocity), while encouraging steady states or small movements towards maintaining stationarity in flight."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:11:43 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    x, y, vx, vy, theta, omega, leg1_contact, leg2_contact = observations\n",
      "    \n",
      "    # Penalize vertical movement and angular velocity to maintain hover\n",
      "    vertical_movement_penalty = -abs(vy)\n",
      "    angular_velocity_penalty = -abs(omega)\n",
      "    \n",
      "    # Penalize horizontal movement\n",
      "    horizontal_movement_penalty = -abs(vx)\n",
      "    \n",
      "    # Penalize if both legs are in contact with the ground (landing)\n",
      "    leg_contact_penalty = -20 * (leg1_contact + leg2_contact)\n",
      "    \n",
      "    # Reward for staying off the ground and stationary\n",
      "    stay_off_ground_reward = 5 * (not leg1_contact) * (not leg2_contact)\n",
      "    hover_reward = 5 * (abs(vy) < 0.1) * (abs(omega) < 0.1) * (abs(vx) < 0.1)\n",
      "    \n",
      "    # Final reward calculation\n",
      "    reward = vertical_movement_penalty + angular_velocity_penalty + horizontal_movement_penalty + leg_contact_penalty + stay_off_ground_reward + hover_reward\n",
      "    \n",
      "    if is_failure:\n",
      "        return -100\n",
      "    \n",
      "    return reward\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:11:54 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: 4.411314010620117\n",
      "\n",
      "22:11:54 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "22:11:54 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700e573520a64da99de589c89f15a373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:12:54 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "22:12:55 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "\n",
      "22:13:02 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_237834-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "22:13:07 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 616521}\n",
      "\n",
      "22:13:07 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 616521}\n",
      "\n",
      "22:13:07 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  1  ########\n",
      "<HELP>\n",
      "\n",
      "**Observation Space Breakdown**\n",
      "\n",
      "The given Observation Space is an 8-dimensional vector representing the state of the lander. The dimensions are:\n",
      "\n",
      "1. x-coordinate (float32) [-2.5, 2.5]\n",
      "2. y-coordinate (float32) [-2.5, 2.5]\n",
      "3. Linear velocity in x-direction (float32) [-10, 10]\n",
      "4. Linear velocity in y-direction (float32) [-10, 10]\n",
      "5. Angle (float32) [-6.2831855, 6.2831855] radians\n",
      "6. Angular velocity (float32) [-10, 10]\n",
      "7. Leg 1 contact boolean (int32) [0, 1]\n",
      "8. Leg 2 contact boolean (int32) [0, 1]\n",
      "\n",
      "The agent's goal is to achieve a stationary flight, meaning it must maintain a constant altitude and airspeed while minimizing the use of legs for support.\n",
      "\n",
      "**Achieving Stationary Flight**\n",
      "\n",
      "To accomplish this task, the agent should employ the following strategies:\n",
      "\n",
      "1. **Altitude Control**: The agent must control the lander's z-coordinate (not explicitly mentioned in the Observation Space) to hover at a desired height. This can be achieved by adjusting the thrust vector of the lander's engines.\n",
      "2. **Airspeed Regulation**: The agent must regulate the linear velocities in x and y directions to maintain a constant airspeed while avoiding collisions with obstacles.\n",
      "3. **Angle Control**: The agent should control the angle (dimension 5) to ensure that the lander remains horizontal or at a desired pitch, which will help in maintaining altitude and stability.\n",
      "4. **Angular Velocity Regulation**: The agent must regulate the angular velocity (dimension 6) to maintain smooth flight, avoiding rapid changes in direction.\n",
      "5. **Leg Contact Minimization**: Since stationary flight implies minimal use of legs for support, the agent should aim to keep both leg contact booleans (dimensions 7 and 8) as low as possible.\n",
      "\n",
      "**Reward Shaping**\n",
      "\n",
      "To encourage the desired behavior, a suitable reward function can be designed to penalize:\n",
      "\n",
      "* Large deviations in altitude\n",
      "* High airspeed changes\n",
      "* Rapid angle changes\n",
      "* Significant angular velocity fluctuations\n",
      "* Frequent leg contact\n",
      "\n",
      "Conversely, rewarding stable flight and minimal leg usage will guide the agent towards achieving stationary flight.\n",
      "\n",
      "**Additional Considerations**\n",
      "\n",
      "The absence of explicit z-coordinate control (dimension 3) might require additional mechanisms, such as altitude measurement or prediction, to be incorporated into the model. Moreover, incorporating wind resistance and other environmental factors can further enhance the realism of the simulation.\n",
      "\n",
      "By implementing these strategies and reward functions, the agent should be able to achieve the desired goal of stationary flight while minimizing crashes and landings.\n",
      "\n",
      "</HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:13:15 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    x_vel = observations[2]\n",
      "    y_vel = observations[3]\n",
      "    angle = observations[4]\n",
      "    ang_vel = observations[5]\n",
      "    leg_contact_1 = observations[6]\n",
      "    leg_contact_2 = observations[7]\n",
      "\n",
      "    reward = 0.0\n",
      "\n",
      "    if is_failure:\n",
      "        return -100.0\n",
      "\n",
      "    # Penalize high linear velocities\n",
      "    reward -= np.abs(x_vel) * 0.5\n",
      "    reward -= np.abs(y_vel) * 0.5\n",
      "\n",
      "    # Penalize non-zero angle and angular velocity\n",
      "    reward -= np.abs(angle) * 0.3\n",
      "    reward -= np.abs(ang_vel) * 0.2\n",
      "\n",
      "    # Penalize leg contact\n",
      "    reward -= (leg_contact_1 + leg_contact_2) * 10.0\n",
      "\n",
      "    return reward"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:13:24 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.13780701078067067\n",
      "\n",
      "22:13:24 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "22:13:24 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1c4b7d71634e14ad07041d9458ff9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:14:23 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "22:14:24 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "22:14:28 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_616521-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "22:14:30 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 781334}\n",
      "\n",
      "22:14:30 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 781334}\n",
      "\n",
      "22:14:30 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  2  ########\n",
      "<HELP>\n",
      "The given observation space defines an 8-dimensional vector that represents the state of the lander. The dimensions are as follows:\n",
      "\n",
      "1. x-coordinate of the lander\n",
      "2. y-coordinate of the lander\n",
      "3. linear velocity in the x-direction (dx)\n",
      "4. linear velocity in the y-direction (dy)\n",
      "5. angle of the lander (θ) measured from the positive x-axis counterclockwise to the projection of the lander's center of mass onto the xy-plane\n",
      "6. angular velocity (ω) of the lander around its vertical axis\n",
      "7. boolean indicating whether the left leg is in contact with the ground (l_leg_contact)\n",
      "8. boolean indicating whether the right leg is in contact with the ground (r_leg_contact)\n",
      "\n",
      "The observation space is a Box space, which means it has both a lower and upper bound for each dimension.\n",
      "\n",
      "To achieve a stationary flight, the agent needs to balance the lander on its legs while moving at a constant velocity without crashing. Here's a step-by-step approach:\n",
      "\n",
      "1. **Initial State**: The lander starts with an initial position (x, y), linear velocities (dx, dy), and angle θ. The goal is to maintain stability.\n",
      "\n",
      "**Action Space:**\n",
      "\n",
      "* For each time step t, the agent can apply one of the following actions:\n",
      "\t+ Thrust (T) in the x-direction (+1 or -1)\n",
      "\t+ Thrust (T) in the y-direction (+1 or -1)\n",
      "\t+ Rotation (R) around the vertical axis (+1 or -1)\n",
      "\n",
      "2. **Control Objectives:**\n",
      "\t* Maintain a stable altitude by controlling the vertical motion (dy) and the rotation (ω).\n",
      "\t* Balance on the legs to prevent crashing, which requires adjusting the lander's position (x, y) and velocity (dx, dy).\n",
      "3. **Reward Function:**\n",
      "\n",
      "The reward function should encourage stable flight while penalizing crashes or loss of altitude:\n",
      "\n",
      "+ Reward for maintaining a constant altitude\n",
      "+ Reward for balancing on the legs (l_leg_contact and r_leg_contact)\n",
      "- Penalty for crashing (when l_leg_contact or r_leg_contact becomes False)\n",
      "- Penalty for losing altitude\n",
      "\n",
      "4. **Learning Process:**\n",
      "\n",
      "The agent learns to achieve stationary flight through trial and error, using reinforcement learning algorithms such as Q-learning or policy gradient methods.\n",
      "\n",
      "**Training Objective:**\n",
      "\t* Minimize the cumulative discounted reward over an episode\n",
      "5. **Policy Iteration:**\n",
      "\n",
      "The agent iteratively updates its policy by exploring different actions in the environment and updating the policy based on the rewards obtained.\n",
      "\n",
      "By following these steps, the agent can learn to achieve a stationary flight while avoiding crashes and maintaining stability.\n",
      "\n",
      "</HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:14:38 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    x, y, dx, dy, angle, angular_velocity, l_leg_contact, r_leg_contact = observations\n",
      "    \n",
      "    altitude_reward = -abs(dy)  # Reward for maintaining a constant altitude\n",
      "    balance_reward = -(l_leg_contact + r_leg_contact) * 0.1  # Penalize contact with the ground\n",
      "    crash_penalty = -50 if is_failure else 0  # Heavy penalty for crashing\n",
      "\n",
      "    return altitude_reward + balance_reward + crash_penalty\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:14:43 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.19085358083248138\n",
      "\n",
      "22:14:43 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "22:14:43 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b811e2499e4e298e720f48f97b7ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:15:37 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "22:15:37 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "22:15:41 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_781334-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "22:15:46 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 15289}\n",
      "\n",
      "22:15:46 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 15289}\n",
      "\n",
      "22:15:46 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  3  ########\n",
      "<HELP>\n",
      "The observation space represents the current state of the environment in which the agent is embedded. It contains 8 dimensions, each with a specific range and data type (float32). \n",
      "\n",
      "- The first two dimensions ([ -2.5, 2.5 ]) represent the x and y coordinates of the lander's position.\n",
      "- The next two dimensions ([ -2.5, 2.5 ]) represent the linear velocities in the x and y directions.\n",
      "- The fifth dimension ([ -6.2831855, 6.2831855 ]) is a cyclic value representing the angle (or orientation) of the lander, often represented as radians for rotational symmetry.\n",
      "- The sixth dimension ([ -10, 10 ]) is another linear velocity but this time for rotation.\n",
      "- The last two dimensions ([ 0, 1 ], and [ 0, 1 ]), represent whether each leg is in contact with the ground or not.\n",
      "\n",
      "For an agent to achieve a stationary flight (not land but also avoid crashing), it must maintain its vertical speed at zero while preventing its descent. It should control its pitch and roll angles carefully so that the downward force exerted by gravity is balanced out by lift forces created through rotation, all this without landing on the ground or hitting any obstacles.\n",
      "\n",
      "Here are the steps an agent can take to achieve stationary flight:\n",
      "1. **Vertical speed control**: The agent must maintain a zero vertical speed (5th dimension) to prevent descending and crashing.\n",
      "2. **Lateral velocity control**: It should keep its lateral velocities small by adjusting the first two dimensions of its state space appropriately, as high speeds can make it difficult to stabilize.\n",
      "3. **Angle control**: Stationary flight requires maintaining a stable pitch angle but also allowing some degree of rotation to balance out the forces created by gravity and lift. Adjusting the 5th dimension within an optimal range will achieve this.\n",
      "4. **Angular velocity control**: The agent must limit its angular velocities (6th dimension) as excessive spinning could destabilize it.\n",
      "5. **Leg contact management**: It's crucial for the agent to avoid letting both legs touch the ground simultaneously, as this would indicate a crash or landing.\n",
      "\n",
      "Achieving stationary flight in such an environment involves balancing multiple competing forces and velocities, making it a highly complex control problem that requires sophisticated algorithms and exploration-exploitation trade-offs.\n",
      "\n",
      "</HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:15:53 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    \"\"\"Reward function for LunarLander-v3\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    x_vel, y_vel = observations[2], observations[3]\n",
      "    angle, angular_vel = observations[4], observations[5]\n",
      "    leg0_contact, leg1_contact = observations[6], observations[7]\n",
      "\n",
      "    if is_success:\n",
      "        return 10.0\n",
      "    if is_failure:\n",
      "        return -10.0\n",
      "\n",
      "    reward = 0.0\n",
      "    reward -= abs(y_vel) * 2.0  # Penalize vertical speed\n",
      "    reward -= abs(x_vel) * 1.5  # Penalize lateral speed\n",
      "    reward -= abs(angle) * 3.0  # Penalize non-zero angle\n",
      "    reward -= abs(angular_vel) * 1.0  # Penalize angular velocity\n",
      "\n",
      "    if leg0_contact or leg1_contact:\n",
      "        reward -= 5.0  # Penalize touching the ground\n",
      "\n",
      "    return reward\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:16:05 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -1.2317349137738347\n",
      "\n",
      "22:16:05 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "22:16:05 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcdff4ac4af4d8a8d364144385c630b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:17:02 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.01\n",
      "\n",
      "22:17:02 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "22:17:07 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_15289-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "22:17:12 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 633998}\n",
      "\n",
      "22:17:12 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 633998}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  4  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:17:12 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HELP>\n",
      "The observation space described is an 8-dimensional vector that represents the state of the lander in various aspects. Let's break down each dimension:\n",
      "\n",
      "1. **x & y coordinates**: The position of the lander in the x and y directions, respectively.\n",
      "2. **Linear velocities (vx & vy)**: The speed at which the lander is moving in the x and y directions, respectively.\n",
      "3. **Angle (θ)**: The orientation of the lander's body, measured from the x-axis counterclockwise.\n",
      "4. **Angular velocity (ω)**: The rate of change of the angle, representing how quickly the lander is rotating.\n",
      "5. **Leg contact (left & right legs)**: Two binary values indicating whether each leg is in contact with the ground or not.\n",
      "\n",
      "To achieve a stationary flight without landing or crashing, the agent must navigate the lander to a stable equilibrium point while maintaining airspeed and avoiding ground impact. Here's a step-by-step approach:\n",
      "\n",
      "1. **Initial state**: The lander starts at a random location with an initial velocity. The agent must quickly assess the situation and adjust its control inputs.\n",
      "2. **Airspeed stabilization**: By adjusting the thrusters, the agent can try to stabilize the airspeed (vx & vy) as close to zero as possible. This is crucial for maintaining flight.\n",
      "3. **Angle adjustment**: The agent must also adjust the angle (θ) of the lander's body to ensure that it remains parallel to the horizon. This will help maintain airspeed and prevent crashes.\n",
      "4. **Angular velocity reduction**: As the airspeed stabilizes, the angular velocity (ω) should be minimized to prevent spinning.\n",
      "5. **Leg contact management**: The agent must carefully manage leg contact with the ground. Since the goal is not to land, the agent should avoid touching down on either leg. If a leg touches the ground, it should immediately lift off before the other leg does.\n",
      "6. **Continuous control adjustments**: Throughout the process, the agent will need to make continuous control adjustments to maintain airspeed, angle, and angular velocity stability.\n",
      "\n",
      "In summary, achieving stationary flight without landing or crashing requires the agent to:\n",
      "\n",
      "* Stabilize airspeed\n",
      "* Adjust angle to horizon\n",
      "* Minimize angular velocity\n",
      "* Manage leg contact with ground\n",
      "\n",
      "To solve this problem using reinforcement learning, a suitable approach would be to use a deep Q-Network (DQN) with a carefully designed reward function that encourages the agent to achieve these goals."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:17:19 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    x_pos, y_pos, vx, vy, angle, angular_velocity, left_leg_contact, right_leg_contact = observations\n",
      "    \n",
      "    if is_failure:\n",
      "        return -10.0\n",
      "    \n",
      "    distance_from_center = np.sqrt(x_pos**2 + y_pos**2)\n",
      "    speed_penalty = abs(vx) + abs(vy)\n",
      "    angle_penalty = abs(angle)\n",
      "    angular_velocity_penalty = abs(angular_velocity)\n",
      "    leg_contact_penalty = left_leg_contact + right_leg_contact\n",
      "    \n",
      "    reward = -0.1 * (distance_from_center + speed_penalty + angle_penalty + angular_velocity_penalty + 5 * leg_contact_penalty)\n",
      "    \n",
      "    return reward\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:17:26 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.25647985914221516\n",
      "\n",
      "22:17:26 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "22:17:26 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bda38a88bd430b997d16da4e82595c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:18:27 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "22:18:27 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "22:18:33 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_633998-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "22:18:38 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 926013}\n",
      "\n",
      "22:18:38 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 926013}\n",
      "\n",
      "22:18:38 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  5  ########\n",
      "<HELP>\n",
      "**Observation Space Description**\n",
      "\n",
      "The given Observation Space is an 8-dimensional vector represented by a Box with the following bounds:\n",
      "\n",
      "* The first two dimensions (0-1) represent the coordinates of the lander in x and y, respectively.\n",
      "* Dimensions 2-3 represent the linear velocities of the lander in x and y, respectively.\n",
      "* Dimension 4 represents the angle of the lander.\n",
      "* Dimension 5 represents the angular velocity of the lander.\n",
      "* Dimensions 6-7 represent two boolean values indicating whether each leg is in contact with the ground or not.\n",
      "\n",
      "In summary, the Observation Space provides a complete state description of the lander, including its position, velocity, orientation, and ground contacts.\n",
      "\n",
      "**Achieving Stationary Flight**\n",
      "\n",
      "To achieve stationary flight without landing but avoiding crashes, the agent must maintain a stable trajectory while controlling its velocity, angle, and angular velocity. Here's a step-by-step guide to help the agent achieve this goal:\n",
      "\n",
      "1. **Initial Positioning**: The lander should be positioned at a safe altitude and distance from obstacles to avoid immediate collisions.\n",
      "2. **Velocity Control**: Gradually reduce the linear velocities in x and y dimensions to a near-zero state, ensuring stability and control over the descent trajectory.\n",
      "3. **Angle and Angular Velocity**: Maintain a constant angle (e.g., 0°) and angular velocity (close to zero) to prevent rotation and maintain orientation.\n",
      "4. **Ground Contact Management**: Ensure that both legs are in contact with the ground by maintaining a stable altitude and adjusting the lander's position accordingly.\n",
      "5. **Altitude Control**: Monitor the altitude and adjust the lander's trajectory to maintain a safe distance from obstacles and terrain features.\n",
      "6. **Stationary Positioning**: Once the desired altitude is reached, gradually reduce the linear velocities in x and y dimensions to zero, allowing the lander to hover stationary.\n",
      "\n",
      "**Key Strategies for Stability**\n",
      "\n",
      "To achieve stationary flight, the agent must balance multiple competing objectives:\n",
      "\n",
      "* Minimize altitude changes (to avoid crashes or excessive energy consumption)\n",
      "* Maintain stable velocity and angular momentum\n",
      "* Control ground contact points to ensure stability and orientation\n",
      "\n",
      "By carefully balancing these factors, the agent can successfully navigate the terrain while maintaining a stationary position.\n",
      "\n",
      "**Challenges and Limitations**\n",
      "\n",
      "Achieving stationary flight in this environment poses significant challenges due to:\n",
      "\n",
      "* The need for precise control over multiple variables (position, velocity, angle, angular velocity)\n",
      "* Limited understanding of terrain features and obstacles\n",
      "* Inadequate sensing capabilities to detect subtle changes in altitude or ground contact\n",
      "\n",
      "Addressing these limitations requires advanced AI techniques, such as reinforcement learning, which can adapt to changing environments and optimize control strategies.\n",
      "\n",
      "</HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:18:45 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    \"\"\"Reward function for LunarLander-v3\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    x_velocity = abs(observations[2])\n",
      "    y_velocity = abs(observations[3])\n",
      "    angle = abs(observations[4])\n",
      "    angular_velocity = abs(observations[5])\n",
      "    leg_contact_left = observations[6]\n",
      "    leg_contact_right = observations[7]\n",
      "\n",
      "    reward = 0.0\n",
      "\n",
      "    if is_failure:\n",
      "        return -10.0\n",
      "\n",
      "    if leg_contact_left or leg_contact_right:\n",
      "        return -5.0\n",
      "\n",
      "    reward -= x_velocity\n",
      "    reward -= y_velocity\n",
      "    reward -= angle * 0.1\n",
      "    reward -= angular_velocity * 0.1\n",
      "\n",
      "    return reward"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:18:55 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.013548573141451926\n",
      "\n",
      "22:18:55 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "22:18:55 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819072a555d2418f8104095fa59d37db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:19:57 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "22:19:57 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "22:20:02 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_926013-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "22:20:07 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 318570}\n",
      "\n",
      "22:20:07 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 318570}\n",
      "\n",
      "22:20:07 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  6  ########\n",
      "<HELP>\n",
      "The observation space represents the state of the environment in which the agent is embedded. It consists of 8 dimensions, each with a specific physical interpretation.\n",
      "\n",
      "1. `x` and `y` coordinates of the lander: These are the spatial positions of the lander within the 2D environment. They range from -2.5 to 2.5, indicating that the lander can move within a square area.\n",
      "2. Linear velocities in `x` and `y`: These dimensions represent the speed at which the lander is moving in each direction. The range of [-10, 10] suggests a high-speed environment where the lander can accelerate or decelerate rapidly.\n",
      "3. Angle: This dimension represents the orientation of the lander within the 2D space. The angle `theta` ranges from `-pi` to `pi`, allowing for rotation and movement in any direction.\n",
      "4. Angular velocity: This dimension represents the rate of change of the angle, indicating how quickly the lander is rotating.\n",
      "5. Two booleans representing leg contact with ground: These binary values (0 or 1) indicate whether each leg is in contact with the ground or not.\n",
      "\n",
      "To achieve a stationary flight without crashing or landing, the agent must carefully control its movement and orientation within this environment. Here's a step-by-step strategy for the agent to accomplish this goal:\n",
      "\n",
      "**Phase 1: Initial Stabilization**\n",
      "\n",
      "* The agent should first stabilize itself by adjusting its angle (`theta`) and angular velocity to zero.\n",
      "* This can be achieved by controlling the thrusters to produce an upward force, counteracting gravity, while also applying torque to adjust the rotation.\n",
      "\n",
      "**Phase 2: Horizontal Movement**\n",
      "\n",
      "* Once stabilized, the agent can begin moving horizontally within a small range (e.g., `x` coordinate between -1 and 1).\n",
      "* To maintain a constant speed and direction, the agent should continuously monitor its linear velocities (`vx` and `vy`) and adjust them accordingly.\n",
      "\n",
      "**Phase 3: Stationary Flight**\n",
      "\n",
      "* The key to achieving stationary flight lies in maintaining a constant altitude while avoiding any forward or backward movement.\n",
      "* To do this, the agent should:\n",
      "\t+ Continuously adjust its angle (`theta`) to maintain an upward orientation.\n",
      "\t+ Regulate its angular velocity to zero to prevent rotation.\n",
      "\t+ Monitor its vertical velocity (`vy`) and adjust it to maintain a stable altitude.\n",
      "\t+ Ensure that at least one leg is always in contact with the ground (boolean `leg_contact_1` or `leg_contact_2` remains 1).\n",
      "\n",
      "**Phase 4: Avoid Crashing/Landing**\n",
      "\n",
      "* To avoid crashing or landing, the agent must continuously monitor its distance from the ground and adjust its position (`x`, `y`) to maintain a safe altitude.\n",
      "* The agent should also be prepared to rapidly change its movement or orientation in response to any unexpected changes in the environment.\n",
      "\n",
      "By carefully controlling these variables, the agent can successfully achieve stationary flight within this challenging environment."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:20:16 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    x, y, vx, vy, angle, angular_velocity, leg_contact_1, leg_contact_2 = observations\n",
      "    if is_failure:\n",
      "        return -10.0\n",
      "    if is_success:\n",
      "        return 5.0\n",
      "    reward = 0.0\n",
      "    reward -= abs(vx) * 0.1  # Penalize horizontal movement\n",
      "    reward -= abs(vy) * 0.2  # Penalize vertical velocity for maintaining altitude\n",
      "    reward -= abs(angle) * 0.3  # Penalize non-zero angle\n",
      "    reward -= abs(angular_velocity) * 0.4  # Penalize rotation\n",
      "    if leg_contact_1 or leg_contact_2:\n",
      "        reward -= 1.0  # Penalize contact with ground\n",
      "    return reward\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:20:24 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.11358299683779478\n",
      "\n",
      "22:20:24 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "22:20:24 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19bbad7d68234fe4bcf98b764463af68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:21:22 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "22:21:23 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "22:21:28 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_318570-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "22:21:30 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 239673}\n",
      "\n",
      "22:21:30 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 239673}\n",
      "\n",
      "22:21:30 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  7  ########\n",
      "<HELP>\n",
      "The observation space describes the 8-dimensional vector that constitutes the state of the lander in the environment. This state is composed of:\n",
      "\n",
      "* The x and y coordinates of the lander, represented by `x` and `y`.\n",
      "* The linear velocities in x and y directions, represented by `vx` and `vy`.\n",
      "* The angle of the lander, represented by `theta`.\n",
      "* The angular velocity, represented by `omega`.\n",
      "* Two boolean values, `leg1_contact` and `leg2_contact`, indicating whether each leg is in contact with the ground or not.\n",
      "\n",
      "To achieve a stationary flight without landing or crashing, the agent must balance its descent while maintaining airspeed. This can be achieved through the following strategies:\n",
      "\n",
      "1.  **Altitude control:** The agent must control the lander's altitude by adjusting the vertical velocity (`vy`). A constant negative `vy` would cause the lander to descend. To maintain a stationary position, the agent should adjust the `vy` to be close to zero.\n",
      "\n",
      "    ```python\n",
      "    vy = 0.0\n",
      "    ```\n",
      "2.  **Horizontal movement:** The agent must control the horizontal velocity (`vx`) to maintain a constant airspeed. A non-zero `vx` will cause the lander to move horizontally, ensuring it remains airborne.\n",
      "\n",
      "    ```python\n",
      "    vx = 0.1\n",
      "    ```\n",
      "3.  **Angle adjustment:** The agent must adjust the angle of attack by changing the value of `theta`. To achieve a stationary flight, the lander's nose should be pointed slightly upwards, so that the `theta` is close to zero.\n",
      "\n",
      "    ```python\n",
      "    theta = 0.1\n",
      "    ```\n",
      "4.  **Angular velocity:** The agent must control the angular velocity (`omega`) to maintain stability. A non-zero `omega` will cause the lander to rotate, which may affect its horizontal movement and altitude control.\n",
      "\n",
      "    ```python\n",
      "    omega = 0.0\n",
      "    ```\n",
      "5.  **Leg contact:** Since the goal is to achieve a stationary flight without landing, both legs should remain off the ground (`leg1_contact` and `leg2_contact` should be False).\n",
      "\n",
      "To summarize, the agent must set its state as follows:\n",
      "\n",
      "*   x: not specified\n",
      "*   y: not specified\n",
      "*   vx: 0.1 (horizontal movement)\n",
      "*   vy: 0.0 (altitude control)\n",
      "*   theta: 0.1 (angle adjustment)\n",
      "*   omega: 0.0 (angular velocity control)\n",
      "*   leg1_contact: False\n",
      "*   leg2_contact: False\n",
      "\n",
      "By adjusting these parameters, the agent can maintain a stationary flight without landing or crashing.</HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:21:37 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    x, y, vx, vy, theta, omega, leg1_contact, leg2_contact = observations\n",
      "    reward = 0.0\n",
      "\n",
      "    if not is_success and not is_failure:\n",
      "        reward -= abs(vy) * 0.5  # Penalize vertical movement\n",
      "        reward += abs(vx) * 0.1  # Reward horizontal movement\n",
      "        reward -= abs(theta) * 0.2  # Penalize angle deviation from horizontal\n",
      "        reward -= abs(omega) * 0.1  # Penalize angular velocity\n",
      "\n",
      "        if leg1_contact or leg2_contact:\n",
      "            reward -= 1.0  # Penalize contact with the ground\n",
      "\n",
      "    return reward\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:21:44 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.19110310189425947\n",
      "\n",
      "22:21:44 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "22:21:44 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf3c9ab73124ec993c72caceffadfd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:22:35 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "22:22:35 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "22:22:38 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_239673-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "22:22:40 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 504541}\n",
      "\n",
      "22:22:40 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 504541}\n",
      "\n",
      "22:22:40 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  8  ########\n",
      "<HELP>\n",
      "\n",
      "**Description of the Observation Space:**\n",
      "\n",
      "The Observation Space represents the state of the environment in which the agent is situated. It is an 8-dimensional vector with the following components:\n",
      "\n",
      "1. `x` and `y` coordinates of the lander (real numbers between -2.5 and 2.5)\n",
      "2. Linear velocities in `x` and `y` directions (real numbers between -10 and 10)\n",
      "3. Angle of the lander (a real number between -6.2831855 and 6.2831855, representing a full circle)\n",
      "4. Angular velocity (a real number between -10 and 0)\n",
      "5. Two booleans indicating whether each leg is in contact with the ground or not\n",
      "\n",
      "**Goal: Achieve Stationary Flight without Landing or Crashing**\n",
      "\n",
      "To achieve stationary flight without landing or crashing, the agent must balance its movement to maintain a stable altitude while preventing contact with the ground.\n",
      "\n",
      "**Key Insights:**\n",
      "\n",
      "1. **Altitude Control**: To avoid crashing, the agent needs to control its vertical velocity (i.e., `y` coordinate) and keep it close to zero.\n",
      "2. **Stationary Movement**: To achieve stationary flight, the agent must balance its horizontal movement (i.e., `x` coordinate). This can be done by maintaining a constant speed in one direction while adjusting the angle of the lander to maintain stability.\n",
      "3. **Angle Adjustment**: The agent needs to adjust its angle (`angle`) to compensate for wind resistance and gravity, ensuring that it stays level and stable.\n",
      "\n",
      "**Approach:**\n",
      "\n",
      "1. **Initial State Estimation**: Estimate the initial state of the environment, including the lander's position, velocity, and angle.\n",
      "2. **Velocity Control**: Use reinforcement learning to learn a control policy that adjusts the lander's linear velocities (`vx` and `vy`) to maintain stable altitude and horizontal movement.\n",
      "3. **Angle Adjustment**: Learn a control policy for adjusting the lander's angle (`angle`) to compensate for wind resistance and gravity, ensuring stability.\n",
      "4. **Contact Avoidance**: Use the two boolean values indicating leg contact with the ground to avoid landing by maintaining a safe distance from the surface.\n",
      "5. **Reward Engineering**: Design a reward function that penalizes crashing and rewards stable flight, providing the agent with feedback on its progress.\n",
      "\n",
      "**Challenges:**\n",
      "\n",
      "1. **High-Dimensional State Space**: The 8-dimensional state space can be challenging for the agent to learn, especially when considering the boolean values indicating leg contact.\n",
      "2. **Non-Stationarity**: The environment may change over time due to factors like wind resistance and gravity, requiring the agent to adapt its control policy.\n",
      "\n",
      "By addressing these challenges through reinforcement learning and reward engineering, it is theoretically possible to train an agent to achieve stationary flight without landing or crashing.</HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:22:48 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    \"\"\"Reward function for LunarLander-v3\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    x, y, vx, vy, angle, ang_v, leg0_contact, leg1_contact = observations\n",
      "    \n",
      "    # Penalize contact with the ground\n",
      "    contact_penalty = -5.0 if (leg0_contact or leg1_contact) else 0.0\n",
      "    \n",
      "    # Reward for maintaining stable altitude (near zero vertical velocity)\n",
      "    altitude_reward = -abs(vy) * 0.2\n",
      "    \n",
      "    # Reward for minimizing horizontal movement\n",
      "    stability_reward = -abs(vx) * 0.2\n",
      "    \n",
      "    # Penalize large angle and angular velocity to maintain balance\n",
      "    angle_penalty = -abs(angle) * 0.1\n",
      "    angular_velocity_penalty = -abs(ang_v) * 0.1\n",
      "    \n",
      "    reward = contact_penalty + altitude_reward + stability_reward + angle_penalty + angular_velocity_penalty\n",
      "    \n",
      "    return reward"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:22:59 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.13542680572718382\n",
      "\n",
      "22:22:59 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "22:22:59 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95a39b7ab8a4fb386e76036b08c7d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:24:02 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "22:24:02 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "22:24:07 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_504541-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "22:24:13 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 187191}\n",
      "\n",
      "22:24:13 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 187191}\n",
      "\n",
      "22:24:13 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  9  ########\n",
      "<HELP>\n",
      "\n",
      "**Observation Space Description**\n",
      "\n",
      "The provided observation space is an 8-dimensional vector representing the state of the lander. It contains:\n",
      "\n",
      "* `x` and `y` coordinates of the lander (floating-point numbers in range [-2.5, 2.5])\n",
      "* Linear velocities `vx` and `vy` of the lander (floating-point numbers in range [-10, 10])\n",
      "* Angle `theta` (radian measure) with a wrap-around behavior due to the use of `6.2831855` as an upper bound, effectively creating a circular space for angle values\n",
      "* Angular velocity `omega` (floating-point number in range [-0, 1])\n",
      "* Two binary booleans `leg1_contact` and `leg2_contact` indicating whether each leg is in contact with the ground or not\n",
      "\n",
      "**Achieving Stationary Flight**\n",
      "\n",
      "To achieve stationary flight without landing but also avoiding a crash, the agent must balance multiple constraints. Here's a step-by-step guide:\n",
      "\n",
      "1. **Control Angle (`theta`)**: To maintain a stable flight, the agent should control the angle `theta` to keep the lander's horizontal velocity (`vy`) near zero. This can be achieved by adjusting the thrust and pitch forces applied to the lander.\n",
      "2. **Control Linear Velocity (`vx`, `vy`)**: The agent must balance the linear velocities to maintain a stable flight trajectory. To avoid landing, it should ensure that the vertical component of velocity (`vy`) remains positive (i.e., upward motion). For stationary flight, both horizontal and vertical components of velocity should be close to zero.\n",
      "3. **Control Angular Velocity (`omega`)**: The agent should control the angular velocity `omega` to maintain a stable rotation around its axis. A low but non-zero angular velocity can help stabilize the lander's attitude.\n",
      "4. **Contact with Ground (Legs)**: To avoid landing, the agent must ensure that both legs are not in contact with the ground simultaneously (`leg1_contact` and `leg2_contact` booleans). This can be achieved by controlling the height of the lander above the ground.\n",
      "5. **Altitude Control**: The agent should control the altitude of the lander to maintain a stable flight trajectory. By adjusting the thrust force, it can increase or decrease the altitude.\n",
      "6. **Stationary Flight Conditions**:\n",
      "\t* `vx` and `vy` should be close to zero (i.e., minimal horizontal and vertical motion)\n",
      "\t* `theta` should be constant, maintaining a stable angle\n",
      "\t* `omega` should be low but non-zero for stability\n",
      "\n",
      "**Control Strategy**: The agent can use the following control strategy:\n",
      "\n",
      "1. **Determine desired velocity**: Set `vx`, `vy`, and `theta` to their desired values.\n",
      "2. **Calculate thrust forces**: Use the desired velocities and angle to calculate the required thrust forces (in x, y, and pitch directions).\n",
      "3. **Apply controls**: Apply the calculated thrust forces to control the lander's movement.\n",
      "\n",
      "**Reward Function**: To encourage stationary flight, a suitable reward function could be designed as follows:\n",
      "\n",
      "* Reward `R` for each time step: `R = 1 - (|vx| + |vy|) / (2 * delta_t)`, where `delta_t` is the time step size. This encourages minimal horizontal and vertical motion.\n",
      "* Penalty `-P` if the lander lands or crashes (`leg1_contact` and/or `leg2_contact` simultaneously).\n",
      "\n",
      "This reward function can be adjusted and combined with other metrics to fine-tune the agent's performance.\n",
      "\n",
      "**Notes**: Achieving stationary flight in this environment is challenging due to the need to balance multiple constraints. The agent must carefully control its velocity, angle, and angular velocity while avoiding landing or crashing.\n",
      "\n",
      "</HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:24:24 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    vx, vy, _, _, _, _, leg1_contact, leg2_contact = observations\n",
      "    velocity_penalty = abs(vx) + abs(vy)\n",
      "    contact_penalty = -50 * (leg1_contact or leg2_contact)\n",
      "    reward = 1 - velocity_penalty / 20 + contact_penalty if not is_failure else -100\n",
      "    return reward"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:24:29 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: 0.9296165585517884\n",
      "\n",
      "22:24:29 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "22:24:29 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fa9e2e274b4f78a2cca34d05261e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22:25:20 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "22:25:20 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "22:25:21 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_187191-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "runs(\n",
    "    total_timesteps=30_000,\n",
    "    nb_vec_envs=1,\n",
    "    nb_refined=0,\n",
    "    human_feedback=False,\n",
    "    video_description=False,\n",
    "    legacy_training=False,\n",
    "    actor_model=\"qwen2.5-coder:32b\",\n",
    "    critic_model=\"llama3.2-vision\",\n",
    "    env=\"LunarLander\",\n",
    "    observation_space=obs_space,\n",
    "    goal=goal,\n",
    "    image=None,\n",
    "    nb_gen=1,\n",
    "    nb_runs=10,\n",
    "    proxies=proxies,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:23:38 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 319551}\n",
      "\n",
      "20:23:38 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 319551}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  0  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:23:56 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HELP>\n",
      "\n",
      "1. **Goal and Annotation:** The red annotation indicates the goal of the game or simulation, which is to land on the ground safely.\n",
      "\n",
      "2. **Meaning:** The image appears to be a screenshot from a video game or simulation where an agent (lander) needs to navigate through space and land on a planet's surface without crashing. The red line marks the boundary between the air and the ground, serving as a visual indicator of the landing zone. \n",
      "\n",
      "3. **Agent Goal:** The agent must successfully navigate to the designated landing site within the marked area and come to rest without any contact with the ground (to avoid crashing), thereby achieving the goal.\n",
      "\n",
      "4. **Observation Space Description:**\n",
      "\n",
      "   *   **Coordinates (x, y):** These are two of the first four components in the observation space vector provided at the end of the prompt.\n",
      "       *   They represent the current position of the lander on the x and y axes of a 2D plane or grid that models the surface it's landing on.\n",
      "\n",
      "   *   **Linear Velocities (vx, vy):** These are the next two components in the vector.\n",
      "       *   They indicate how fast the lander is moving in the x and y directions at any given time, which is crucial for control and navigation as it affects how the agent interacts with its environment.\n",
      "\n",
      "   *   **Angle:** This component likely refers to the orientation of the lander relative to true north or some other fixed reference point. It's essential for understanding the lander's position and aligning itself properly with the landing site.\n",
      "\n",
      "   *   **Angular Velocity:** This represents how fast the lander is turning around its vertical axis, influencing both its trajectory and stability upon impact.\n",
      "\n",
      "   *   **Boolean Flags for Legs in Contact:** These two final components are boolean (true/false) values indicating whether each of the lander's legs is currently touching the ground. This information is vital for understanding if the agent has successfully landed or not.\n",
      "\n",
      "</HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:24:27 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    x, y, vx, vy, angle, angular_velocity, leg0_contact, leg1_contact = observations\n",
      "    reward = 0.0\n",
      "    \n",
      "    if is_success:\n",
      "        reward += 100.0\n",
      "    elif is_failure:\n",
      "        reward -= 100.0\n",
      "    else:\n",
      "        reward -= (abs(vx) + abs(vy)) * 0.1  # penalize high velocities\n",
      "        reward -= abs(angle) * 0.5  # penalize non-zero angle\n",
      "        reward -= abs(angular_velocity) * 0.2  # penalize angular velocity\n",
      "    \n",
      "    return reward\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:24:33 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.08298461390659213\n",
      "\n",
      "20:24:33 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "20:24:33 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2248ee014e74116ae3e6013aed3bd8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:25:25 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.01\n",
      "\n",
      "20:25:25 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:25:31 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_319551-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:25:33 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 307547}\n",
      "\n",
      "20:25:33 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 307547}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  1  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:25:34 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Step 1: Answering the Question**\n",
      "\n",
      "*   Red Annotation: The red annotation points to a small blue square located at the center of the black rectangle, which reads \"STATIONARY FLIGHT\".\n",
      "*   Meaning: This indicates that the goal of the agent is to achieve stationary flight.\n",
      "\n",
      "**Step 2: Determining the Goal**\n",
      "\n",
      "Based on the red annotation, the agent needs to accomplish stationary flight by:\n",
      "\n",
      "*   Maintaining a stable position in mid-air\n",
      "*   Avoiding any significant movement or descent\n",
      "\n",
      "**Step 3: Describing the Observation Space**\n",
      "\n",
      "The observation space comprises an 8-dimensional vector that provides information about the lander's state. This includes:\n",
      "\n",
      "*   Coordinates (x, y) of the lander\n",
      "*   Linear velocities (vx, vy) in x and y directions\n",
      "*   Angle (θ) representing the direction the lander is facing\n",
      "*   Angular velocity (ω) indicating how quickly the lander is rotating\n",
      "*   Two booleans for each leg, signifying whether they are in contact with the ground or not\n",
      "\n",
      "This detailed observation space enables the agent to make informed decisions based on its current state and surroundings.\n",
      "\n",
      "<HELP>\n",
      "The red annotation points to \"STATIONARY FLIGHT\", indicating that the goal of the agent is to achieve stationary flight. The observation space consists of an 8-dimensional vector including coordinates, linear velocities, angle, angular velocity, and leg contact status.\n",
      "</HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:25:40 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    \"\"\"Reward function for LunarLander-v3\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    x, y, vx, vy, angle, angular_velocity, leg1_contact, leg2_contact = observations\n",
      "    stationary_reward = -np.abs(vx) - np.abs(vy) - np.abs(angular_velocity)\n",
      "    if is_success:\n",
      "        return 100.0 + stationary_reward\n",
      "    elif is_failure:\n",
      "        return -100.0\n",
      "    else:\n",
      "        return stationary_reward"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:25:48 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.8374297022819519\n",
      "\n",
      "20:25:48 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "20:25:48 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ea96864c9841ba8f29db56b6781f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:26:48 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "20:26:48 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:26:53 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_307547-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:26:59 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 399215}\n",
      "\n",
      "20:26:59 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 399215}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  2  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:27:00 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HELP>\n",
      "\n",
      "The red annotation in the image represents a goal for an agent to achieve, which is to land safely on the ground while minimizing the impact of the landing.\n",
      "\n",
      "The observation space is defined by the vector:\n",
      "\n",
      "[ -2.5 -2.5 -10. -10. -6.2831855 -10. -0. -0. ]\n",
      "\n",
      "This 8-dimensional vector represents various aspects of the state, including:\n",
      "* Land coordinates: x and y\n",
      "* Linear velocities in x and y directions\n",
      "* Angle (measured in radians)\n",
      "* Angular velocity\n",
      "* Two booleans indicating whether each leg is touching the ground\n",
      "\n",
      "These attributes provide a comprehensive understanding of the lander's position and movement within the environment.\n",
      "\n",
      "</HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:27:03 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    \"\"\"Reward function for LunarLander-v3\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    x, y, vx, vy, angle, angular_v, leg0_contact, leg1_contact = observations\n",
      "    reward = 0.0\n",
      "    \n",
      "    if is_success:\n",
      "        reward += 200.0\n",
      "    elif is_failure:\n",
      "        reward -= 50.0\n",
      "    else:\n",
      "        reward -= np.abs(vx) * 0.05\n",
      "        reward -= np.abs(vy) * 0.1\n",
      "        reward -= np.abs(angle) * 0.1\n",
      "        reward += (leg0_contact + leg1_contact) * 10.0\n",
      "    \n",
      "    return reward\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:27:13 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.038757152017205955\n",
      "\n",
      "20:27:13 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "20:27:13 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1701e1f33c54fe69e4bd2e53236260a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:28:01 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "20:28:01 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:28:02 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_399215-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:28:04 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 42158}\n",
      "\n",
      "20:28:04 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 42158}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  3  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:28:04 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**<HELP> Step 1: Annotation Identification and Meaning </HELP>**\n",
      "\n",
      "The red annotation in the image is a visual representation of the goal that an agent needs to achieve. The annotation consists of three main elements:\n",
      "\n",
      "*   A small square object, likely representing the lander.\n",
      "*   An arrow pointing upwards towards the top-center of the image.\n",
      "\n",
      "**<HELP> Step 2: Goal Interpretation </HELP>**\n",
      "\n",
      "Based on this visual representation, we can interpret the goal as follows:\n",
      "\n",
      "The agent's objective is to guide the lander to reach the top-center of the image. This suggests that the agent needs to navigate the lander from its current position towards a target location at the top-center.\n",
      "\n",
      "**<Observation Space Description>**\n",
      "\n",
      "**Step 3: Observation Space Overview**\n",
      "\n",
      "The provided code snippet outlines the structure of the observation space, which represents the environment state observed by the agent:\n",
      "\n",
      "```python\n",
      "Box([ -2.5 -2.5 -10. -10. -6.2831855 -10. -0. -0. ], \n",
      "[ 2.5 2.5 10. 10. 6.2831855 10. 1. 1. ], (8,), float32)\n",
      "```\n",
      "\n",
      "**Step 4: Breakdown of Observation Space Components**\n",
      "\n",
      "The observation space consists of an 8-dimensional vector, representing various aspects of the lander's state:\n",
      "\n",
      "*   **x and y coordinates**: The first two components represent the position of the lander in a 2D plane, with ranges from -2.5 to 2.5.\n",
      "*   **Linear velocities (x and y)**: The next two components indicate the velocity of the lander along the x and y axes, ranging from -10 to 10.\n",
      "*   **Angle**: The fifth component represents the angle of the lander, measured in radians within a range of -6.2831855 to 6.2831855 (pi).\n",
      "*   **Angular velocity**: The sixth component indicates the rate of change of the lander's angle, with a range from -10 to 10.\n",
      "*   **Boolean flags for leg contact**: The last two components are boolean values indicating whether each of the legs is in contact with the ground.\n",
      "\n",
      "This breakdown provides a clear understanding of the different aspects that comprise the observation space and their respective ranges."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:28:15 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    \"\"\"Reward function for LunarLander-v3\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    x, y, vx, vy, angle, angvel, leg1_contact, leg2_contact = observations\n",
      "    distance_to_target = np.sqrt((x**2 + (y - 1.5)**2))\n",
      "    \n",
      "    if is_success:\n",
      "        return 1000.0\n",
      "    elif is_failure:\n",
      "        return -1000.0\n",
      "    \n",
      "    reward = -distance_to_target\n",
      "    reward -= 0.01 * abs(vx) + 0.01 * abs(vy)\n",
      "    reward -= 0.05 * abs(angle)\n",
      "    \n",
      "    if leg1_contact or leg2_contact:\n",
      "        reward -= 0.1\n",
      "    \n",
      "    return reward"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:28:25 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.0813239119461679\n",
      "\n",
      "20:28:25 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "20:28:25 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55bc7eb221b548fd888e1493b1ff113b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:29:20 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "20:29:20 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:29:26 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_42158-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:29:31 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 337675}\n",
      "\n",
      "20:29:31 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 337675}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  4  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:29:32 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HELP>\n",
      "\n",
      "**The Goal**\n",
      "* Annotation: The red annotation highlights the goal of the environment.\n",
      "* Meaning: The goal is to land the spacecraft on a celestial body.\n",
      "\n",
      "**Agent's Objective**\n",
      "* Based on the goal, the agent needs to achieve:\n",
      "\t+ Successful landing\n",
      "\t+ Stabilize the spacecraft's position and velocity\n",
      "\t+ Avoid crashes or collisions\n",
      "\n",
      "**Observation Space Description**\n",
      "* 8-dimensional vector comprising:\n",
      "\t1. **x & y coordinates**: Position of the lander in 2D space.\n",
      "\t2. **Linear velocities (x & y)**: Velocity components in horizontal plane.\n",
      "\t3. **Angle**: Orientation of the lander relative to the surface.\n",
      "\t4. **Angular velocity**: Rate of change of the angle.\n",
      "\t5. **Boolean indicators (2)**: Contact status with ground legs.\n",
      "\n",
      "</HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:29:36 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    \"\"\"Reward function for LunarLander-v3\"\"\"\n",
      "    \n",
      "    if is_success:\n",
      "        return 100.0\n",
      "    elif is_failure:\n",
      "        return -100.0\n",
      "    \n",
      "    x, y, vx, vy, angle, angvel, leg_contact_left, leg_contact_right = observations\n",
      "    reward = 0.0\n",
      "    \n",
      "    # Penalize high velocities\n",
      "    reward -= abs(vx) * 0.2\n",
      "    reward -= abs(vy) * 0.5\n",
      "    \n",
      "    # Penalize tilting\n",
      "    reward -= abs(angle) * 0.1\n",
      "    \n",
      "    # Reward for being close to the ground and having legs in contact\n",
      "    if y < -0.5:\n",
      "        reward += (1 - abs(y)) * 2\n",
      "        if leg_contact_left or leg_contact_right:\n",
      "            reward += 1.0\n",
      "    \n",
      "    return reward\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:29:45 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.08500163648277521\n",
      "\n",
      "20:29:45 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "20:29:45 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8683efb22c4c48e6999e1abf40c78b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:30:35 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.01\n",
      "\n",
      "20:30:36 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:30:38 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_337675-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:30:40 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 468194}\n",
      "\n",
      "20:30:40 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 468194}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  5  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:30:41 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Step 1: Understand the Environment**\n",
      "\n",
      "The image depicts a \"STATIONARY FLIGHT\" environment where an agent is required to land safely on a planet's surface.\n",
      "\n",
      "*   **Goal:** The red annotation indicates that the goal of the environment is to \"LANDER\".\n",
      "*   **Meaning:** This means that the agent needs to successfully land on the ground without any obstacles or hindrances.\n",
      "\n",
      "**Step 2: Determine the Goal Based on Observation Space**\n",
      "\n",
      "The observation space provides information about the state of the lander in an 8-dimensional vector. This includes:\n",
      "\n",
      "*   Coordinates (x, y) of the lander\n",
      "*   Linear velocities (vx, vy) of the lander\n",
      "*   Angle and angular velocity of the lander\n",
      "*   Two booleans indicating whether each leg is in contact with the ground\n",
      "\n",
      "**Step 3: Conclusion**\n",
      "\n",
      "Based on the observation space, the goal of the agent is to successfully land on the planet's surface while ensuring that all legs are firmly grounded.\n",
      "\n",
      "<HELP>\n",
      "\n",
      "The environment consists of a \"STATIONARY FLIGHT\" scenario where an agent must land safely on a planet's surface. The goal is explicitly stated as \"LANDER\", indicating that the agent needs to achieve a safe landing without obstacles or hindrances. The observation space provides essential information about the lander's state in an 8-dimensional vector, including its coordinates, linear velocities, angle, angular velocity, and leg contact status. Therefore, the primary goal of the agent is to successfully land on the planet's surface while ensuring that all legs are firmly grounded.\n",
      "\n",
      "</HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:30:49 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    x, y, vx, vy, angle, angular_velocity, leg0_contact, leg1_contact = observations\n",
      "    if is_success:\n",
      "        return 100.0\n",
      "    elif is_failure:\n",
      "        return -100.0\n",
      "    else:\n",
      "        reward = -np.abs(vx) * 0.5 - np.abs(vy) * 2.0 - np.abs(angle) * 0.5 - np.abs(angular_velocity) * 0.1\n",
      "        if leg0_contact or leg1_contact:\n",
      "            reward += 5.0\n",
      "        return reward"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:30:55 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -1.048632656596601\n",
      "\n",
      "20:30:55 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "20:30:55 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1edff2cdb3c543498a18f35bdb29ef16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:31:43 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "20:31:43 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:31:45 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_468194-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:31:47 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 55253}\n",
      "\n",
      "20:31:47 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 55253}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  6  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:31:48 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HELP>\n",
      "To achieve the goal of landing safely on the moon's surface, the agent must navigate through an environment with a series of obstacles and hazards while managing its speed, direction, and altitude.\n",
      "\n",
      "The red annotation \"STATIONARY FLIGHT\" indicates that the agent needs to maintain a stable position in space and avoid any rapid movements or changes in orientation. This requires careful control over the lander's thrusters and attitude control system to ensure smooth and controlled descent.\n",
      "\n",
      "</HELP>\n",
      "\n",
      "<HELP>\n",
      "The Observation Space consists of 8 dimensions, which are:\n",
      "\n",
      "*   **x**: The x-coordinate of the lander.\n",
      "*   **y**: The y-coordinate of the lander.\n",
      "*   **vx**: The linear velocity in the x-direction.\n",
      "*   **vy**: The linear velocity in the y-direction.\n",
      "*   **theta**: The angle of the lander.\n",
      "*   **omega**: The angular velocity.\n",
      "*   **leg1_contact**: A boolean indicating whether the first leg is in contact with the ground.\n",
      "*   **leg2_contact**: A boolean indicating whether the second leg is in contact with the ground.\n",
      "\n",
      "These dimensions provide the agent with a comprehensive understanding of its surroundings, allowing it to make informed decisions about navigation and control. By analyzing these observations, the agent can determine its position, velocity, orientation, and interaction with the environment, ultimately enabling it to achieve the goal of landing safely on the moon's surface.</HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:31:54 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    \"\"\"Reward function for LunarLander-v3\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    x, y, vx, vy, theta, omega, leg1_contact, leg2_contact = observations\n",
      "    \n",
      "    if is_success:\n",
      "        return 100.0\n",
      "    elif is_failure:\n",
      "        return -100.0\n",
      "    \n",
      "    # Penalize high velocity and angular velocity to encourage stationary flight\n",
      "    velocity_penalty = np.sqrt(vx**2 + vy**2)\n",
      "    angle_penalty = abs(theta) * (1 - int(leg1_contact or leg2_contact))\n",
      "    \n",
      "    # Reward for being close to the ground but not too close\n",
      "    height_reward = 1.0 / (1.0 + y)\n",
      "    \n",
      "    return -velocity_penalty - 5 * angle_penalty + height_reward\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:32:04 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.46803128824371737\n",
      "\n",
      "20:32:04 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "20:32:04 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a99200e18c49f5ab7506b632c98568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:32:56 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.01\n",
      "\n",
      "20:32:56 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:33:01 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_55253-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:33:05 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 964795}\n",
      "\n",
      "20:33:05 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 964795}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  7  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:33:06 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**<HELP> Description of the red annotation:**\n",
      "\n",
      "*   The red line surrounds an image of a purple lander, with an arrow pointing at it.\n",
      "*   It is accompanied by text in red font that says \"STATIONARY FLIGHT.\"\n",
      "\n",
      "**<HELP> Meaning of the red annotation:**\n",
      "\n",
      "The red annotation indicates that the goal of the reinforcement learning agent is to achieve stationary flight. The agent must navigate the lander to a stable position where it can maintain its altitude and velocity without falling or rising.\n",
      "\n",
      "**<HELP> Goal for the agent:**\n",
      "\n",
      "The agent's primary objective is to successfully execute a stationary landing, which means avoiding any loss of control or stability during descent.\n",
      "\n",
      "**<HELP> Description of the Observation Space:**\n",
      "\n",
      "*   The observation space contains an 8-dimensional vector that represents the state of the lander.\n",
      "*   The first four dimensions correspond to the coordinates and velocities of the lander in x and y directions.\n",
      "*   The fifth dimension represents the angle of the lander.\n",
      "*   The sixth dimension corresponds to the angular velocity of the lander.\n",
      "*   The last two dimensions are binary indicators for each leg, indicating whether they are in contact with the ground or not.\n",
      "\n",
      "This comprehensive state representation allows the agent to make informed decisions about how to control the lander and achieve its goal."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:33:12 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    \"\"\"Reward function for LunarLander-v3\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    x, y, vx, vy, angle, angular_velocity, leg_contact_left, leg_contact_right = observations\n",
      "    \n",
      "    # Base reward for staying in the air and moving slowly\n",
      "    base_reward = -0.1 * (abs(vx) + abs(vy))\n",
      "    \n",
      "    # Penalty for tilting too much\n",
      "    tilt_penalty = -5 * abs(angle)\n",
      "    \n",
      "    # Reward for having both legs touching the ground (stable landing attempt)\n",
      "    leg_contact_reward = 2 if leg_contact_left and leg_contact_right else 0\n",
      "    \n",
      "    # Success reward\n",
      "    success_reward = 100 if is_success else 0\n",
      "    \n",
      "    # Failure penalty\n",
      "    failure_penalty = -150 if is_failure else 0\n",
      "    \n",
      "    return base_reward + tilt_penalty + leg_contact_reward + success_reward + failure_penalty"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:33:23 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.14930407498031856\n",
      "\n",
      "20:33:23 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "20:33:23 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e2402461854f1eb7de4bc7041f0273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:34:12 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.05\n",
      "\n",
      "20:34:12 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:34:13 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_964795-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:34:15 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 228071}\n",
      "\n",
      "20:34:15 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 228071}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  8  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:34:16 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a signpost pointing left and right in an area without any landmarks, which may be challenging to navigate due to its size.\n",
      "This observation space indicates that the agent is located within a large, unmarked area with no visible landmarks.\n",
      "\n",
      "**Observation Space Breakdown**\n",
      "\n",
      "* **Coordinates**: The agent's position is represented by two real numbers between -2.5 and 2.5, indicating it can move freely in a 2D space.\n",
      "* **Linear Velocities**: Two more real numbers (-0 to 1) represent the agent's speed and direction of movement along the x-axis (left or right).\n",
      "* **Angle**: A single real number between -10 and 10 represents the angle the agent is facing.\n",
      "* **Angular Velocity**: The rate at which the agent turns is represented by another real number between -0.5 and 0.5.\n",
      "* **Leg Contact**: Two boolean values indicate whether each leg (left or right) is in contact with the ground, helping the agent plan its next move.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "The Observation Space provides a rich set of information about the agent's position, movement, orientation, and interactions with the environment, enabling the agent to navigate and interact effectively."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:34:21 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    x_pos = observations[0]\n",
      "    y_pos = observations[1]\n",
      "    x_vel = observations[2]\n",
      "    y_vel = observations[3]\n",
      "    angle = observations[4]\n",
      "    ang_vel = observations[5]\n",
      "    left_leg_contact = observations[6]\n",
      "    right_leg_contact = observations[7]\n",
      "\n",
      "    reward = 0.0\n",
      "\n",
      "    if is_success:\n",
      "        reward += 100.0\n",
      "    elif is_failure:\n",
      "        reward -= 20.0\n",
      "\n",
      "    # Penalize for being far from the origin\n",
      "    reward -= np.sqrt(x_pos**2 + y_pos**2) / 5.0\n",
      "\n",
      "    # Reward for landing on the ground with both legs in contact\n",
      "    if left_leg_contact and right_leg_contact:\n",
      "        reward += 10.0\n",
      "\n",
      "    # Penalize for high velocity at landing\n",
      "    reward -= (np.abs(x_vel) + np.abs(y_vel)) * 2.0\n",
      "\n",
      "    # Penalize for being tilted\n",
      "    reward -= np.abs(angle) / 2.0\n",
      "\n",
      "    return reward\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:34:32 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -1.8904907893397405\n",
      "\n",
      "20:34:32 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "20:34:32 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5fb1be44324411915ab91aeb67cc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:35:18 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "20:35:19 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:35:20 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_228071-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:35:21 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 795773}\n",
      "\n",
      "20:35:21 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 795773}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  9  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:35:22 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Step 1: Answering the questions about the red annotation**\n",
      "\n",
      "*   The red annotation is pointing to the text \"STATIONARY FLIGHT\" in the image.\n",
      "*   The meaning of this text is that the agent needs to achieve stationary flight.\n",
      "\n",
      "**Step 2: Determining the goal of the agent based on the answer**\n",
      "\n",
      "Based on the answer, the goal of the agent is to achieve stationary flight.\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "<HELP>\n",
      "The red annotation points to the text \"STATIONARY FLIGHT\", which means that the agent needs to achieve stationary flight. The Observation Space consists of 8 dimensions: x and y coordinates of the lander, its linear velocities in x and y directions, its angle, its angular velocity, and two booleans indicating whether each leg is in contact with the ground or not.\n",
      "</HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:35:26 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    \"\"\"Reward function for LunarLander-v3\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    x_velocity = observations[2]\n",
      "    y_velocity = observations[3]\n",
      "    angle = observations[4]\n",
      "    angular_velocity = observations[5]\n",
      "\n",
      "    stationary_reward = -abs(x_velocity) - abs(y_velocity) - abs(angle) - abs(angular_velocity)\n",
      "    success_bonus = 100 if is_success else 0\n",
      "    failure_penalty = -100 if is_failure else 0\n",
      "\n",
      "    return stationary_reward + success_bonus + failure_penalty"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:35:34 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -1.566845178604126\n",
      "\n",
      "20:35:34 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "20:35:34 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17b17e2a0554df6bd08c672f273f776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "20:36:32 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "20:36:32 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "20:36:37 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_795773-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "runs(\n",
    "    total_timesteps=30_000,\n",
    "    nb_vec_envs=1,\n",
    "    nb_refined=0,\n",
    "    human_feedback=False,\n",
    "    video_description=False,\n",
    "    legacy_training=False,\n",
    "    actor_model=\"qwen2.5-coder:32b\",\n",
    "    critic_model=\"llama3.2-vision\",\n",
    "    env=\"LunarLander\",\n",
    "    observation_space=obs_space,\n",
    "    goal=None,\n",
    "    image=image,\n",
    "    nb_gen=1,\n",
    "    nb_runs=10,\n",
    "    proxies=proxies,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text+Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:04:49 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 631760}\n",
      "\n",
      "21:04:49 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 631760}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  0  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:05:00 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Step 1: Understand the Environment**\n",
      "\n",
      "The environment is a challenging task where an agent must land a spacecraft on Mars while avoiding crashing. The state space consists of eight dimensions:\n",
      "\n",
      "*   $x$ and $y$ coordinates\n",
      "*   Linear velocities in $x$ and $y$\n",
      "*   Angle and angular velocity\n",
      "*   Two booleans indicating whether each leg is in contact with the ground or not\n",
      "\n",
      "**Step 2: Identify the Goal**\n",
      "\n",
      "The goal is to achieve a stationary flight, which means the agent must navigate the spacecraft to hover in place without crashing.\n",
      "\n",
      "**Step 3: Analyze the Observation Space**\n",
      "\n",
      "From the observation space provided earlier:\n",
      "\n",
      "<HELP> The observation space consists of eight dimensions, including the x and y coordinates of the lander, its linear velocities in these directions, its angle and angular velocity, and two booleans indicating whether each leg is in contact with the ground or not. This suggests that the agent has a good understanding of its spatial position and orientation within the environment. </HELP>\n",
      "\n",
      "**Step 4: Determine How to Achieve the Goal**\n",
      "\n",
      "To achieve the goal of stationary flight:\n",
      "\n",
      "<HELP> To accomplish this, the agent needs to learn how to maintain a stable altitude while controlling its angular velocity and ensuring that no leg is in contact with the ground at any point during flight. This will require careful management of the spacecraft's thrust vectors, navigation, and orientation control systems. The agent should focus on developing strategies for stabilizing the spacecraft and preventing contact between legs and the ground, thereby achieving a stationary flight state. </HELP>\n",
      "\n",
      "**Step 5: Provide Recommendations for the Agent**\n",
      "\n",
      "Based on the analysis:\n",
      "\n",
      "<HELP> To achieve stationary flight, it is recommended that the agent focus on developing strategies for stabilizing the spacecraft, controlling its angular velocity, and ensuring no leg is in contact with the ground at any point during flight. This will involve careful management of thrust vectors, navigation, and orientation control systems to maintain a stable altitude while preventing ground contacts. By mastering these skills, the agent should be able to successfully achieve a stationary flight state within the environment. </HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:05:16 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    x, y, vx, vy, angle, angular_velocity, leg0_contact, leg1_contact = observations\n",
      "    hover_bonus = 1.0 if abs(vx) < 0.5 and abs(vy) < 0.5 and abs(angle) < np.pi / 6 else 0.0\n",
      "    no_crash_penalty = -1.0 if is_failure else 0.0\n",
      "    leg_contact_penalty = -0.5 * (leg0_contact + leg1_contact)\n",
      "    return hover_bonus + no_crash_penalty + leg_contact_penalty\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:05:22 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: 0.0\n",
      "\n",
      "21:05:22 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "21:05:22 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4ec2372d544cf580c76b32ca9aeebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:06:20 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "21:06:20 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "\n",
      "21:06:25 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_631760-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "21:06:30 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 570404}\n",
      "\n",
      "21:06:30 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 570404}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  1  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:06:31 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**<HELP> Environment Description and Goal Achievement</HELP>**\n",
      "\n",
      "The image illustrates an environment where an agent controls a lander's movement to achieve a stationary flight.\n",
      "\n",
      "**Key Components:**\n",
      "\n",
      "*   **Lander Position**: The x and y coordinates of the lander, represented as 2D vectors within the range [-2.5, -2.5] to [2.5, 2.5].\n",
      "*   **Linear Velocities**: The x and y components of the lander's velocity vector, allowing it to move horizontally or vertically.\n",
      "*   **Angle**: The angle at which the lander is oriented, enabling control over its direction.\n",
      "*   **Angular Velocity**: A component that influences how quickly the lander changes its orientation.\n",
      "*   **Leg Contact**: Two boolean values indicating whether each leg is in contact with the ground or not, affecting stability.\n",
      "\n",
      "**Goal: Stationary Flight**\n",
      "\n",
      "The objective is to maintain a stable position while avoiding impact on the surface. This requires balancing the lander's movement and orientation to prevent collision.\n",
      "\n",
      "**Achieving the Goal:**\n",
      "\n",
      "1.  **Initial Positioning**: Place the lander at the center of the environment, ensuring optimal control over its movements.\n",
      "2.  **Stabilize Orientation**: Set the angle and angular velocity to maintain a stable position, preventing rotation that could lead to loss of contact with the ground or collision.\n",
      "3.  **Adjust Velocities**: Manage linear velocities to achieve minimal movement, thereby maintaining a stationary state while avoiding impact.\n",
      "4.  **Leg Control**: Ensure both legs are in contact with the ground by adjusting their positions and velocities accordingly, stabilizing the lander's stance.\n",
      "\n",
      "By carefully controlling these parameters, the agent can successfully execute a stationary flight within this environment, meeting the specified goal without crashing or failing to land."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:06:39 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    x, y, vx, vy, angle, angular_v, leg0_contact, leg1_contact = observations\n",
      "    if is_success:\n",
      "        return 10.0\n",
      "    elif is_failure:\n",
      "        return -10.0\n",
      "    else:\n",
      "        reward = 1.0\n",
      "        reward -= abs(vx) + abs(vy)  # Penalize linear movement\n",
      "        reward -= abs(angular_v)     # Penalize rotation\n",
      "        if leg0_contact or leg1_contact:  # Penalize contact with the ground\n",
      "            reward -= 2.0\n",
      "        return reward\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:06:45 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: 0.1431272253394127\n",
      "\n",
      "21:06:45 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "21:06:45 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce75ba784634d03823bab753d2c9803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:07:43 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "21:07:44 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "21:07:49 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_570404-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "21:07:54 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 359613}\n",
      "\n",
      "21:07:54 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 359613}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  2  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:07:55 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HELP>The red annotation in the image represents the landing site for the lander. The Observation Space consists of an 8-dimensional vector that provides information about the lander's state, including its position, velocity, angle, and angular velocity, as well as two booleans indicating whether each leg is in contact with the ground.\n",
      "\n",
      "To achieve a stationary flight without crashing or landing, the agent needs to maintain a stable altitude while avoiding obstacles. This can be achieved by:\n",
      "\n",
      "1. **Stabilizing the lander's orientation**: The agent should ensure that the lander remains upright and maintains a consistent angle.\n",
      "2. **Controlling velocity**: The agent must carefully manage the lander's linear and angular velocities to prevent it from crashing into the ground or other objects.\n",
      "3. **Avoiding obstacles**: The agent should use its sensors to detect obstacles in the environment and adjust its trajectory accordingly to avoid collisions.\n",
      "4. **Maintaining altitude**: The agent needs to keep the lander at a safe distance from the ground, adjusting its position and velocity as needed to maintain a stable altitude.\n",
      "\n",
      "By carefully managing these factors, the agent can achieve a stationary flight without crashing or landing.</HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:08:00 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    x, y, vx, vy, angle, angular_v, leg1_contact, leg2_contact = observations\n",
      "    distance_from_ground = abs(y)\n",
      "    reward = 0.0\n",
      "\n",
      "    if is_success:\n",
      "        return -100.0  # Penalize for unintended landing\n",
      "\n",
      "    if is_failure:\n",
      "        return -100.0  # Penalize for crashing\n",
      "\n",
      "    if leg1_contact or leg2_contact:\n",
      "        reward -= 50.0  # Penalize for touching the ground without landing\n",
      "\n",
      "    reward += distance_from_ground * 0.1  # Reward for maintaining altitude\n",
      "    reward -= abs(vx) + abs(vy)  # Penalize for high linear velocity\n",
      "    reward -= abs(angle) * 2.0  # Penalize for tilting\n",
      "    reward -= abs(angular_v)  # Penalize for angular movement\n",
      "\n",
      "    return reward"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:08:09 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -1.197715673968196\n",
      "\n",
      "21:08:09 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "21:08:09 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5d494b7ce249d2bad1a088cf7f63ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:09:06 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "21:09:06 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "21:09:11 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_359613-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "21:09:16 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 178757}\n",
      "\n",
      "21:09:16 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 178757}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  3  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:09:18 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HELP>The environment described is a complex scenario for an agent to navigate. The state space consists of eight dimensions, each representing a key aspect of the lander's status:\n",
      "\n",
      "*   **x and y coordinates**: These define the lander's position on a 2D plane.\n",
      "*   **Linear velocities in x and y directions**: This indicates how fast the lander is moving horizontally.\n",
      "*   **Angle**: The orientation of the lander relative to the vertical axis.\n",
      "*   **Angular velocity**: How quickly the angle changes, indicating any rotation around its vertical axis.\n",
      "*   **Two booleans for leg contact with ground**: These binary indicators show whether each of the two legs is currently in contact with the surface.\n",
      "\n",
      "The task involves achieving stationary flight without crashing or landing. This means the agent must balance the lander so that it does not move downward but also prevents it from crashing into the ground or losing altitude.\n",
      "\n",
      "To achieve this, the agent would need to apply appropriate control inputs (like thrust adjustments) to maintain a stable position and orientation in space while avoiding collision with any surface. Achieving stationary flight requires careful balance of forces to prevent descent while ensuring that the lander does not move in any direction at a significant speed.\n",
      "\n",
      "This task is challenging due to the high dimensionality of the state space, requiring precise control over multiple variables simultaneously. It demands expertise in dynamic systems and control theory, as well as advanced machine learning techniques capable of handling complex nonlinear interactions between different state components.</HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:09:24 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    \"\"\"Reward function for LunarLander-v3\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    x_pos, y_pos, x_vel, y_vel, angle, ang_vel, leg0_contact, leg1_contact = observations\n",
      "    \n",
      "    # Penalize large velocities and angular velocity to encourage stationary flight\n",
      "    vel_penalty = np.abs(x_vel) + np.abs(y_vel)\n",
      "    angvel_penalty = np.abs(ang_vel)\n",
      "    \n",
      "    # Reward for maintaining a stable angle close to vertical (0 radians)\n",
      "    angle_reward = 1 - abs(angle)\n",
      "    \n",
      "    # Penalize if any leg touches the ground\n",
      "    contact_penalty = -(leg0_contact or leg1_contact) * 1.5\n",
      "    \n",
      "    # Base reward\n",
      "    base_reward = -vel_penalty - angvel_penalty + angle_reward + contact_penalty\n",
      "    \n",
      "    return base_reward\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:09:35 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: 0.7197707844898105\n",
      "\n",
      "21:09:35 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "21:09:35 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab36e084c084bc089d53bb18e2f6c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:10:32 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "21:10:33 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "21:10:38 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_178757-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "21:10:43 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 210114}\n",
      "\n",
      "21:10:43 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 210114}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  4  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:10:44 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HELP> The red annotation in the image represents the agent's goal, which is to maintain a stable and controlled flight while avoiding a hard landing or crash. This requires careful management of the agent's state, including its position, velocity, angle, angular velocity, and leg contact status.\n",
      "\n",
      "The Observation Space provides an 8-dimensional vector that describes the current state of the lander:\n",
      "\n",
      "* x and y coordinates: These represent the horizontal position of the lander within a 2D environment.\n",
      "* Linear velocities in x and y directions: These indicate the speed at which the lander is moving horizontally.\n",
      "* Angle: This measures the orientation of the lander relative to its surroundings.\n",
      "* Angular velocity: This indicates the rate at which the lander's angle changes.\n",
      "* Two booleans representing leg contact status: These indicate whether each leg is in contact with the ground or not.\n",
      "\n",
      "To achieve a stationary flight, the agent must learn to balance and stabilize itself while preventing hard landings. This involves adjusting its thrust, orientation, and velocity to maintain equilibrium within the designated flight zone. The key challenges include:\n",
      "\n",
      "1. Positioning and Orientation: Maintaining precise control over x-y coordinates and angle ensures the lander remains stable.\n",
      "2. Velocity Management: Carefully managing linear and angular velocities prevents excessive movement or rotation that could lead to instability.\n",
      "3. Leg Contact Monitoring: Avoiding hard landings requires monitoring leg contact status closely, ensuring proper orientation and landing technique.\n",
      "\n",
      "By mastering these aspects of the Observation Space through training and reinforcement learning algorithms, the agent can achieve the goal of maintaining a stationary flight while avoiding hard landings or crashes.</HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:10:51 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    x, y, vx, vy, angle, angular_velocity, leg_contact_left, leg_contact_right = observations\n",
      "    \n",
      "    position_reward = -0.1 * (abs(x) + abs(y))\n",
      "    velocity_penalty = -0.5 * (vx**2 + vy**2)\n",
      "    angle_penalty = -0.1 * abs(angle)\n",
      "    angular_velocity_penalty = -0.05 * abs(angular_velocity)\n",
      "    contact_penalty = -1.0 if leg_contact_left or leg_contact_right else 0.0\n",
      "    failure_penalty = -10.0 if is_failure else 0.0\n",
      "    \n",
      "    return position_reward + velocity_penalty + angle_penalty + angular_velocity_penalty + contact_penalty + failure_penalty"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:10:58 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.16599155413503994\n",
      "\n",
      "21:10:58 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "21:10:58 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfdf5b8320c423398b7945f7ee4e76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:11:56 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "21:11:56 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "21:12:02 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_210114-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "21:12:07 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 442119}\n",
      "\n",
      "21:12:07 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 442119}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  5  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:12:08 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HELP> The red annotation in the image represents the 8-dimensional vector that defines the state of the lander, which is the observation space. This includes:\n",
      "\n",
      "* Coordinates (x, y) of the lander\n",
      "* Linear velocities (vx, vy) in x and y directions\n",
      "* Angle of rotation\n",
      "* Angular velocity\n",
      "* Two booleans indicating whether each leg is in contact with the ground or not\n",
      "\n",
      "This means that the agent has to maintain a stable position and orientation while avoiding crashing into the ground.\n",
      "\n",
      "To achieve a stationary flight without landing or crashing, the agent needs to balance its control inputs to keep itself airborne while preventing it from touching the ground. This requires precise control of the lander's movements, as any misstep could result in collision with the ground.\n",
      "\n",
      "The key to this challenge is understanding the relationship between the state variables and how they affect the lander's dynamics. By carefully analyzing the observation space, the agent can infer which actions are likely to maintain a stable flight trajectory.\n",
      "\n",
      "In essence, the agent needs to find an equilibrium point that balances its linear and angular velocities with its position and orientation. This involves complex calculations and simulations to predict the outcomes of different control inputs.\n",
      "\n",
      "Ultimately, achieving stationary flight in this environment requires mastering the art of balancing competing forces and constraints while navigating through a challenging dynamic system. It demands expertise in both classical control theory and modern reinforcement learning techniques, highlighting the need for sophisticated algorithms that can adapt to unpredictable environments.\n",
      "\n",
      "This thought-provoking problem encourages us to appreciate the intricacies involved in controlling complex systems like spacecraft landers, emphasizing the importance of rigorous mathematical modeling and simulation in developing effective strategies for overcoming such challenges.</HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:12:15 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    \"\"\"Reward function for LunarLander-v3\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    x, y, vx, vy, angle, angular_velocity, leg0_contact, leg1_contact = observations\n",
      "    \n",
      "    # Penalize contact with the ground to avoid landing or crashing\n",
      "    contact_penalty = -5.0 if (leg0_contact or leg1_contact) else 0.0\n",
      "    \n",
      "    # Reward for being near the center of the screen and having low velocities\n",
      "    position_reward = -np.abs(x) * 0.25 - np.abs(y - 5) * 0.25\n",
      "    velocity_penalty = -np.abs(vx) * 0.1 - np.abs(vy) * 0.1\n",
      "    \n",
      "    # Penalize for angular movement to maintain stability\n",
      "    angle_reward = -np.abs(angle) * 0.1 - np.abs(angular_velocity) * 0.1\n",
      "    \n",
      "    reward = position_reward + velocity_penalty + angle_reward + contact_penalty\n",
      "    \n",
      "    return reward"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:12:27 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.9883070994634181\n",
      "\n",
      "21:12:27 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "21:12:27 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278b4fd9a3244707aa9764b5fafaf100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:13:13 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "21:13:13 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "21:13:15 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_442119-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "21:13:16 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 458415}\n",
      "\n",
      "21:13:16 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 458415}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  6  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:13:17 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HELP>The image shows a box plot of the observation space in an environment where a spacecraft (lander) is navigating through a vacuum. The red annotation highlights that the state is an 8-dimensional vector representing different aspects of the lander's state.\n",
      "\n",
      "*   **Land X and Y Coordinates**: These represent the position of the lander on the x-axis (-2.5 to 2.5) and y-axis (-10 to 10), respectively.\n",
      "*   **Linear Velocities in X and Y Directions**: These indicate the speed at which the lander is moving along the x-axis (in meters per second, from -6.2831855 to 6.2831855) and y-axis (from -10 to 10).\n",
      "*   **Angle and Angular Velocity**: The angle (from -0 to 1) represents the orientation of the lander relative to its initial position or direction of travel. The angular velocity (also from -0 to 1) indicates how quickly this orientation is changing.\n",
      "*   **Leg Contact with Ground**: Two boolean values (0 or 1, indicating False or True) represent whether each leg is in contact with the ground, respectively.\n",
      "\n",
      "Given this information and understanding that achieving a stationary flight without landing involves maintaining control over speed and direction while ensuring no impact with the ground:\n",
      "\n",
      "**Strategies for Stationary Flight:**\n",
      "\n",
      "*   **Control of Speed**: The agent must learn to maintain or adjust its linear velocities (x and y directions) within an optimal range. This could involve stabilizing the lander's velocity around a desired value, preventing it from getting too close to 0 (stopping) or reaching too high values (increasing risk of crash).\n",
      "*   **Control of Direction**: The angle and angular velocity components are crucial for directing the spacecraft. The agent must learn to adjust these parameters to ensure the lander remains on course without drifting away.\n",
      "*   **Ground Contact Management**: Since landing should be avoided, ensuring at least one leg is always in contact with the ground (Boolean value 1) becomes a critical strategy. This helps stabilize the spacecraft by providing an anchor point against gravity and movement forces.\n",
      "*   **Avoidance of Crash**: Monitoring all state components to avoid exceeding safe thresholds for linear velocities, angle, and angular velocity is essential. If any component approaches unsafe values, the agent must adjust its control inputs accordingly.\n",
      "\n",
      "By effectively managing these parameters through appropriate actions (e.g., thruster control), the spacecraft can achieve stationary flight, balancing stability with the need to prevent landing or crashing. This strategy requires precise control and continuous monitoring of the environment to make real-time adjustments.\n",
      "\n",
      "**<End HELP>**"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:13:29 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    x_velocity = observations[2]\n",
      "    y_velocity = observations[3]\n",
      "    angle = observations[4]\n",
      "    angular_velocity = observations[5]\n",
      "    leg0_contact = observations[6]\n",
      "    leg1_contact = observations[7]\n",
      "\n",
      "    if is_failure:\n",
      "        return -1.0\n",
      "    elif is_success:\n",
      "        return 0.0\n",
      "\n",
      "    reward = 1.0\n",
      "    reward -= np.abs(x_velocity) * 0.1\n",
      "    reward -= np.abs(y_velocity) * 0.1\n",
      "    reward -= np.abs(angle) * 0.5\n",
      "    reward -= np.abs(angular_velocity) * 0.2\n",
      "    if leg0_contact or leg1_contact:\n",
      "        reward -= 0.5\n",
      "\n",
      "    return max(reward, -0.1)\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:13:36 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: 0.9336813307832926\n",
      "\n",
      "21:13:36 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "21:13:36 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c169709aee84360b81c9b70a67a71bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:14:22 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "21:14:22 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "21:14:23 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_458415-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "21:14:24 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 215642}\n",
      "\n",
      "21:14:24 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 215642}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  7  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:14:25 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### <HELP>Observation Space Annotation</HELP>\n",
      "\n",
      "The image depicts the observation space for an agent tasked with landing a rover on a planet. The red annotation highlights three key components:\n",
      "\n",
      "*   **Position and Velocity**: The top-left corner shows the agent's current position (x, y) and velocity (\\v_x,\\v_y). This information enables the agent to understand its spatial relationship with the environment.\n",
      "*   **Angle and Angular Velocity**: Below this are the angle of rotation (θ) and angular velocity (\\\\omega\\) around the z-axis. This provides insight into the agent's orientation and rotational dynamics.\n",
      "*   **Leg Contact Information**: The bottom-right corner contains two booleans: \"is\\_left\\_leg\\_on\\_ground\" and \"is\\_right\\_leg\\_on\\_ground\". These flags indicate whether each leg is in contact with the ground, influencing the agent's control strategies.\n",
      "\n",
      "### <HELP>Observation Space Description</HELP>\n",
      "\n",
      "The observation space consists of 8 dimensions:\n",
      "\n",
      "1.  **x**: Position along the x-axis\n",
      "2.  **y**: Position along the y-axis\n",
      "3.  **\\v_x**: Linear velocity in the x-direction\n",
      "4.  **\\v_y**: Linear velocity in the y-direction\n",
      "5.  **θ**: Angle of rotation around the z-axis\n",
      "6.  **\\omega**: Angular velocity around the z-axis\n",
      "7.  **is\\_left\\_leg\\_on\\_ground**: Boolean indicating left leg contact with the ground\n",
      "8.  **is\\_right\\_leg\\_on\\_ground**: Boolean indicating right leg contact with the ground\n",
      "\n",
      "### <HELP>Stationary Flight Goal</HELP>\n",
      "\n",
      "To achieve a stationary flight without landing, the agent must:\n",
      "\n",
      "1.  **Maintain Altitude**: Keep its position above the ground while avoiding descent.\n",
      "2.  **Regulate Velocity**: Manage its linear and angular velocities to prevent crashes or falls.\n",
      "3.  **Control Orientation**: Stabilize its rotation around the z-axis to maintain a steady orientation.\n",
      "4.  **Manage Leg Contact**: Ensure that at least one leg remains in contact with the ground, providing stability and preventing loss of control.\n",
      "\n",
      "By carefully manipulating these factors, the agent can achieve a stationary flight state where it hovers above the surface without landing or crashing.\n",
      "\n",
      "### <HELP>Key Takeaways</HELP>\n",
      "\n",
      "*   The observation space provides essential information for controlling the rover's movement.\n",
      "*   A stationary flight requires balancing altitude, velocity, orientation, and leg contact to avoid landing or crashing."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:14:36 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    x, y, vx, vy, angle, angular_velocity, left_contact, right_contact = observations\n",
      "    altitude_reward = 1.0 / (y + 1) if y > 0 else -1.0\n",
      "    velocity_penalty = np.abs(vx) + np.abs(vy)\n",
      "    rotation_penalty = np.abs(angle) + np.abs(angular_velocity)\n",
      "    leg_contact_penalty = left_contact + right_contact\n",
      "    reward = altitude_reward - velocity_penalty - rotation_penalty - leg_contact_penalty\n",
      "    if is_failure:\n",
      "        return -10.0\n",
      "    elif is_success:\n",
      "        return 0.0\n",
      "    else:\n",
      "        return reward"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:14:42 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.6629781448439136\n",
      "\n",
      "21:14:42 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "21:14:42 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62dfdd89d25546888b565ddd8e7df2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:15:41 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "21:15:41 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "21:15:46 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_215642-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "21:15:52 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 143855}\n",
      "\n",
      "21:15:52 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 143855}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  8  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:15:52 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HELP> The red annotation in the image represents the \"STATIONARY FLIGHT\" environment, which is an 8-dimensional vector that includes the coordinates of the lander's position (x and y), its linear velocities (dx and dy), its angle ($\\theta$) and angular velocity ($\\omega$), as well as two booleans indicating whether each leg is in contact with the ground or not. The agent's goal is to achieve a stationary flight, meaning it needs to maintain its position and orientation while avoiding crashes.\n",
      "\n",
      "To achieve this goal, the agent can use a combination of control policies that adjust the lander's thrusters and legs accordingly. One possible approach is to use a reinforcement learning algorithm such as Q-learning or deep Q-networks (DQN) to learn an optimal policy for controlling the lander. The algorithm would receive feedback in the form of rewards or penalties based on how well the agent performs, allowing it to iteratively update its policy until it converges to an optimal solution.\n",
      "\n",
      "Additionally, the agent could use sensor readings from sensors such as accelerometers, gyroscopes, and magnetometers to estimate its state and adjust its control inputs accordingly. This would help improve the accuracy of its predictions and enable it to make more informed decisions about when to apply thrust or adjust its orientation.\n",
      "\n",
      "Overall, achieving a stationary flight in this environment requires careful consideration of the lander's dynamics, sensor readings, and reward structure, as well as the development of an effective control policy through reinforcement learning. </HELP>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:15:59 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    \"\"\"Reward function for LunarLander-v3\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    x, y, dx, dy, angle, angular_velocity, leg0_contact, leg1_contact = observations\n",
      "    position_reward = -np.abs(dx) - np.abs(dy)\n",
      "    angle_reward = -np.abs(angle)\n",
      "    contact_penalty = -5 * (leg0_contact + leg1_contact)\n",
      "    failure_penalty = -20 if is_failure else 0\n",
      "    success_reward = 10 if is_success else 0\n",
      "\n",
      "    return position_reward + angle_reward + contact_penalty + failure_penalty + success_reward"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:16:07 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.8062577843666077\n",
      "\n",
      "21:16:07 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "21:16:07 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32459376b537487c870cb35450cd2571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:17:06 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "21:17:06 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "21:17:11 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_143855-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "21:17:16 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert specializing in the design of python reward functions.\n",
      "        Strict criteria:\n",
      "        1. Take care of Generate ALWAYS DIFFERENTS reward function per Response iteration\n",
      "        2. Complete ONLY the reward function code\n",
      "        3. Give no additional explanations\n",
      "        4. STOP immediately your completion after the last return\n",
      "        5. Assuming Numpy already imported as np\n",
      "        6. Take into the observation of the state, the is_success boolean flag, the is_failure boolean flag\n",
      "        , Options: {'temperature': 0.9, 'seed': 600632}\n",
      "\n",
      "21:17:16 OllamaChat.py:39 \u001b[92mINFO\u001b[0m\n",
      "\tSystem: \n",
      "        You're a reinforcement learning expert, great mathematician and physician.\n",
      "        As a critic, you're going to explains step by step, the environment.\n",
      "        Every answer you make will be contained in an xml <HELP> tag., Options: {'temperature': 0.9, 'seed': 600632}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######  9  ########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:17:17 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse -1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**<HELP> Understanding the Environment and Goal </HELP>**\n",
      "\n",
      "### Step 1: Environment Description\n",
      "\n",
      "*   The environment is described in the Observation Space section. It's an 8-dimensional vector comprising:\n",
      "    *   Coordinates of the lander in x & y axes\n",
      "    *   Linear velocities in x & y axes\n",
      "    *   Angle (rotation) of the lander\n",
      "    *   Angular velocity (rate of rotation)\n",
      "    *   Two boolean values indicating contact with each leg on the ground\n",
      "\n",
      "### Step 2: Red Annotation Interpretation\n",
      "\n",
      "The red annotation \"STATIONARY FLIGHT\" indicates a specific goal for the agent. It suggests that instead of landing or crashing, the agent should maintain a stationary position in flight.\n",
      "\n",
      "### Step 3: Scientific Knowledge Application\n",
      "\n",
      "To achieve this goal, consider the following steps:\n",
      "\n",
      "1.  **Maintain Angle and Angular Velocity:** The agent must control its angle (rotation) and angular velocity to stabilize itself in mid-air without rotation.\n",
      "2.  **Balance Velocities:** Ensure linear velocities are minimized or set to zero if possible, to prevent movement towards landing or crashing.\n",
      "3.  **Leg Contact Management:** Manage boolean values related to leg contact by lifting them off the ground when necessary to avoid landing and ensure free flight.\n",
      "4.  **Continuous Feedback Loop:** The agent needs a continuous feedback loop from sensors or observations about its state and position relative to the ground to make adjustments in real-time.\n",
      "\n",
      "### Step 4: Conclusion\n",
      "\n",
      "Achieving stationary flight involves maintaining stability, balancing movements, and strategically managing leg contact. By leveraging precise control over angle, angular velocity, and linear velocities, the agent can successfully accomplish the goal of stationary flight without landing or crashing."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:17:25 OllamaChat.py:195 \u001b[92mINFO\u001b[0m\n",
      "\tResponse 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    \"\"\"Reward function for LunarLander-v3\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    x_velocity = abs(observations[2])\n",
      "    y_velocity = abs(observations[3])\n",
      "    angle = abs(observations[4])\n",
      "    angular_velocity = abs(observations[5])\n",
      "    leg_contact_1 = observations[6]\n",
      "    leg_contact_2 = observations[7]\n",
      "\n",
      "    if is_success:\n",
      "        return -1.0\n",
      "    elif is_failure:\n",
      "        return -1.0\n",
      "\n",
      "    reward = 0.0\n",
      "    reward -= x_velocity * 0.01\n",
      "    reward -= y_velocity * 0.01\n",
      "    reward -= angle * 0.05\n",
      "    reward -= angular_velocity * 0.1\n",
      "    if leg_contact_1 or leg_contact_2:\n",
      "        reward -= 0.1\n",
      "\n",
      "    return reward\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:17:36 GenCode.py:228 \u001b[94mDEBUG\u001b[0m\n",
      "\tReward function output: -0.021538926512002947\n",
      "\n",
      "21:17:36 PolicyTrainer.py:60 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 begin is learning\n",
      "\n",
      "21:17:36 PolicyTrainer.py:319 \u001b[94mDEBUG\u001b[0m\n",
      "\tsimple env\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e084961d42dc4b5cbf564e37e96f9716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "21:18:30 PolicyTrainer.py:80 \u001b[92mINFO\u001b[0m\n",
      "\tstate 1 has finished learning with performances: 0.0\n",
      "\n",
      "21:18:31 PolicyTrainer.py:152 \u001b[92mINFO\u001b[0m\n",
      "\tthe threshold is 0.9\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "21:18:33 VIRAL.py:217 \u001b[92mINFO\u001b[0m\n",
      "\tvideo safe at: records/LunarLander-v3/LunarLander-v3_600632-last.mp4\n",
      "/home/valentin/.conda/envs/llm/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/valentin/Travail/VIRAL/src/records/LunarLander-v3 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "runs(\n",
    "    total_timesteps=30_000,\n",
    "    nb_vec_envs=1,\n",
    "    nb_refined=0,\n",
    "    human_feedback=False,\n",
    "    video_description=False,\n",
    "    legacy_training=False,\n",
    "    actor_model=\"qwen2.5-coder:32b\",\n",
    "    critic_model=\"llama3.2-vision\",\n",
    "    env=\"LunarLander\",\n",
    "    observation_space=obs_space,\n",
    "    goal=goal,\n",
    "    image=image,\n",
    "    nb_gen=1,\n",
    "    nb_runs=10,\n",
    "    proxies=proxies,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
