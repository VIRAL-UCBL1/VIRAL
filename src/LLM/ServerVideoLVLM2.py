import os
import warnings
from time import sleep

import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

warnings.filterwarnings("ignore")
import re
import subprocess

from flask import Flask, jsonify, request

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'video'


def _execute_ollama_ps():
    """
    Execute the command 'ollama ps' to get the list of running models
    and extract the first model name from the output.
    
    Returns:
        str: The name of the first running model or None if no model is running."""
    try:
        result = subprocess.run(
            ["ollama", "ps"], capture_output=True, text=True, check=True
        )
        output = result.stdout
        print("stdout 'ollama ps':")
        print(output)

        match = re.search(
            r"NAME\s+ID\s+SIZE\s+PROCESSOR\s+UNTIL\s+(.+?)\s+", output, re.DOTALL
        )

        if match:
            first_name = match.group(1).strip()
            print(f"ollama model : {first_name}")
            return first_name
        else:
            print("ollama ps return None")
            return None

    except subprocess.CalledProcessError as e:
        print(f"Error : {e}")
        print(f"Stderr: {e.stderr}")
        return None


def _execute_ollama_stop(model: str):
    """
    Execute the command 'ollama stop' to stop the specified model.
    
    Args:
        model (str): The name of the model to stop.
    """
    try:
        result = subprocess.run(
            ["ollama", "stop", model], capture_output=True, text=True, check=True
        )
        output = result.stdout
        print("stdout 'ollama stop':")
        print(output)
        sleep(1)

    except subprocess.CalledProcessError as e:
        print(f"Error : {e}")
        print(f"Stderr: {e.stderr}")

@app.route('/upload', methods=['POST'])
def upload_video():
    """
    Uploads a video file to the server.
    
    Returns:
        str: A message indicating the success of the video upload.
    """
    if 'file' not in request.files:
        return 'Video not in the request'
    file = request.files['file']
    filename = 'tmp.mp4'
    file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))
    print('upload_video filename: ' + filename)
    return 'Video successfully uploaded'

@app.route("/", methods=["POST"])
def process_video():
    """
    Process the uploaded video and generate a response to the user's question.
    
    Returns:
        str: The response generated by the model.
    """
    # default: Load the model on the available device(s)
    # model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    #     "Qwen/Qwen2.5-VL-7B-Instruct", torch_dtype="auto", device_map="auto"
    # )
    # We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        "Qwen/Qwen2.5-VL-7B-Instruct",
        torch_dtype=torch.bfloat16,
        attn_implementation="flash_attention_2",
        device_map="auto",
    )
    model.eval()

    # default processer
    processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct", use_fast=True)
    # The default range for the number of visual tokens per image in the model is 4-16384.
    # You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.
    # min_pixels = 256*28*28
    # max_pixels = 1280*28*28
    # processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels)
    print("load processor")

    # Inputs
    data = request.get_json()
    video_path = './video/tmp.mp4'
    fps = 15
    question = data['prompt']
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "video",
                    "video": video_path,
                    "fps": fps,
                },
                {"type": "text", "text": question},
            ],
        }
    ]
    text = processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    image_inputs, video_inputs = process_vision_info(messages)
    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        fps=fps,
        padding=True,
        return_tensors="pt",
    )
    inputs = inputs.to("cuda")
    print("inputs load")

    ollama_model = _execute_ollama_ps()
    while ollama_model is not None:
        _execute_ollama_stop(ollama_model)
        ollama_model = _execute_ollama_ps()

    print("begin generate")
    with torch.no_grad():
        # Inference: Generation of the output
        generated_ids = model.generate(**inputs, max_new_tokens=128)
        generated_ids_trimmed = [
            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
        print(output)

    response = {'response': output}
    del model
    del inputs
    del image_inputs
    del video_inputs
    del generated_ids
    torch.cuda.empty_cache()
    return jsonify(response)

if __name__ == "__main__":
    app.run(debug=True)
