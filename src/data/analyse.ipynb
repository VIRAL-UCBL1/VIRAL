{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward(data: pd.DataFrame) -> None:\n",
    "\tfor index, row in data[['rewards', 'sr']].iterrows():\n",
    "\t\trew = row['rewards']\n",
    "\t\tsr = row['sr']\n",
    "\t\tplt.plot(rew, alpha=0.7)\n",
    "\tplt.legend()\n",
    "\tplt.ylabel('normalized cumulative reward')\n",
    "\tplt.xlabel('episodes')\n",
    "\tplt.title('CartPole')\n",
    "\tplt.suptitle('Comparison of learning converge speeds')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>env</th>\n",
       "      <th>llm</th>\n",
       "      <th>llm_param</th>\n",
       "      <th>algo</th>\n",
       "      <th>algo_param</th>\n",
       "      <th>total_timesteps</th>\n",
       "      <th>reward_function</th>\n",
       "      <th>rewards</th>\n",
       "      <th>mean_reward</th>\n",
       "      <th>std_reward</th>\n",
       "      <th>sr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/model/CartPole-v1_56776_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 56776}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.0013466388, 0.006040589, 0.0059295106, 0.00...</td>\n",
       "      <td>0.030907</td>\n",
       "      <td>0.036511</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/model/CartPole-v1_56776_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 56776}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.0013466388, 0.006040589, 0.0059295106, 0.00...</td>\n",
       "      <td>0.030907</td>\n",
       "      <td>0.036511</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/model/CartPole-v1_175710_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 175710}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.00426373, 0.005035245, 0.011928752, 0.01813...</td>\n",
       "      <td>0.030089</td>\n",
       "      <td>0.046499</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/model/CartPole-v1_175710_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 175710}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.00426373, 0.005035245, 0.011928752, 0.01813...</td>\n",
       "      <td>0.030089</td>\n",
       "      <td>0.046499</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/model/CartPole-v1_867808_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 867808}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.00983197, 0.021463675, 0.003928835, 0.00234...</td>\n",
       "      <td>0.032096</td>\n",
       "      <td>0.043715</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>data/model/CartPole-v1_365666_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 365666}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.006858737, 0.00078279676, 0.0024521227, 0.0...</td>\n",
       "      <td>0.031481</td>\n",
       "      <td>0.047054</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>data/model/CartPole-v1_610988_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 610988}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.0030825841, 0.008896213, 0.0022266477, 0.00...</td>\n",
       "      <td>0.029146</td>\n",
       "      <td>0.047805</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>data/model/CartPole-v1_610988_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 610988}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.0034331265, 0.009535602, 0.0025809805, 0.00...</td>\n",
       "      <td>0.029250</td>\n",
       "      <td>0.047031</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>data/model/CartPole-v1_401884_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 401884}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[-0.016892578, -0.017205993, -0.014144864, -0....</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>0.051902</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>data/model/CartPole-v1_401884_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 401884}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[-0.016892578, -0.017205993, -0.014144864, -0....</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>0.051902</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   path          env  \\\n",
       "0    data/model/CartPole-v1_56776_1.pth  CartPole-v1   \n",
       "1    data/model/CartPole-v1_56776_2.pth  CartPole-v1   \n",
       "2   data/model/CartPole-v1_175710_1.pth  CartPole-v1   \n",
       "3   data/model/CartPole-v1_175710_2.pth  CartPole-v1   \n",
       "4   data/model/CartPole-v1_867808_1.pth  CartPole-v1   \n",
       "..                                  ...          ...   \n",
       "95  data/model/CartPole-v1_365666_2.pth  CartPole-v1   \n",
       "96  data/model/CartPole-v1_610988_1.pth  CartPole-v1   \n",
       "97  data/model/CartPole-v1_610988_2.pth  CartPole-v1   \n",
       "98  data/model/CartPole-v1_401884_1.pth  CartPole-v1   \n",
       "99  data/model/CartPole-v1_401884_2.pth  CartPole-v1   \n",
       "\n",
       "                              llm         llm_param algo  \\\n",
       "0   qwen2.5-coder_llama3.2-vision   {'seed': 56776}  PPO   \n",
       "1   qwen2.5-coder_llama3.2-vision   {'seed': 56776}  PPO   \n",
       "2   qwen2.5-coder_llama3.2-vision  {'seed': 175710}  PPO   \n",
       "3   qwen2.5-coder_llama3.2-vision  {'seed': 175710}  PPO   \n",
       "4   qwen2.5-coder_llama3.2-vision  {'seed': 867808}  PPO   \n",
       "..                            ...               ...  ...   \n",
       "95  qwen2.5-coder_llama3.2-vision  {'seed': 365666}  PPO   \n",
       "96  qwen2.5-coder_llama3.2-vision  {'seed': 610988}  PPO   \n",
       "97  qwen2.5-coder_llama3.2-vision  {'seed': 610988}  PPO   \n",
       "98  qwen2.5-coder_llama3.2-vision  {'seed': 401884}  PPO   \n",
       "99  qwen2.5-coder_llama3.2-vision  {'seed': 401884}  PPO   \n",
       "\n",
       "                                           algo_param  total_timesteps  \\\n",
       "0   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "1   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "2   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "3   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "4   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "..                                                ...              ...   \n",
       "95  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "96  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "97  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "98  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "99  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "\n",
       "                                      reward_function  \\\n",
       "0   def reward_func(observations:np.ndarray, is_su...   \n",
       "1   def reward_func(observations:np.ndarray, is_su...   \n",
       "2   def reward_func(observations:np.ndarray, is_su...   \n",
       "3   def reward_func(observations:np.ndarray, is_su...   \n",
       "4   def reward_func(observations:np.ndarray, is_su...   \n",
       "..                                                ...   \n",
       "95  def reward_func(observations:np.ndarray, is_su...   \n",
       "96  def reward_func(observations:np.ndarray, is_su...   \n",
       "97  def reward_func(observations:np.ndarray, is_su...   \n",
       "98  def reward_func(observations:np.ndarray, is_su...   \n",
       "99  def reward_func(observations:np.ndarray, is_su...   \n",
       "\n",
       "                                              rewards  mean_reward  \\\n",
       "0   [0.0013466388, 0.006040589, 0.0059295106, 0.00...     0.030907   \n",
       "1   [0.0013466388, 0.006040589, 0.0059295106, 0.00...     0.030907   \n",
       "2   [0.00426373, 0.005035245, 0.011928752, 0.01813...     0.030089   \n",
       "3   [0.00426373, 0.005035245, 0.011928752, 0.01813...     0.030089   \n",
       "4   [0.00983197, 0.021463675, 0.003928835, 0.00234...     0.032096   \n",
       "..                                                ...          ...   \n",
       "95  [0.006858737, 0.00078279676, 0.0024521227, 0.0...     0.031481   \n",
       "96  [0.0030825841, 0.008896213, 0.0022266477, 0.00...     0.029146   \n",
       "97  [0.0034331265, 0.009535602, 0.0025809805, 0.00...     0.029250   \n",
       "98  [-0.016892578, -0.017205993, -0.014144864, -0....     0.012780   \n",
       "99  [-0.016892578, -0.017205993, -0.014144864, -0....     0.012780   \n",
       "\n",
       "    std_reward    sr  \n",
       "0     0.036511  0.73  \n",
       "1     0.036511  0.73  \n",
       "2     0.046499  1.00  \n",
       "3     0.046499  1.00  \n",
       "4     0.043715  0.09  \n",
       "..         ...   ...  \n",
       "95    0.047054  1.00  \n",
       "96    0.047805  1.00  \n",
       "97    0.047031  1.00  \n",
       "98    0.051902  0.04  \n",
       "99    0.051902  0.05  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('CartPole_v1_log.csv', delimiter=';')\n",
    "data['rewards'] = data['rewards'].map(lambda x : np.array(list(map(float, x.split(',')))))\n",
    "#data = data.where(data['sr']>0.90).dropna()\n",
    "data.where(data['llm'] == 'qwen2.5-coder_llama3.2-vision')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>env</th>\n",
       "      <th>llm</th>\n",
       "      <th>llm_param</th>\n",
       "      <th>algo</th>\n",
       "      <th>algo_param</th>\n",
       "      <th>total_timesteps</th>\n",
       "      <th>reward_function</th>\n",
       "      <th>rewards</th>\n",
       "      <th>mean_reward</th>\n",
       "      <th>std_reward</th>\n",
       "      <th>sr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/model/CartPole-v1_56776_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 56776}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.0013466388, 0.006040589, 0.0059295106, 0.00...</td>\n",
       "      <td>0.030907</td>\n",
       "      <td>0.036511</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/model/CartPole-v1_56776_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 56776}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.0013466388, 0.006040589, 0.0059295106, 0.00...</td>\n",
       "      <td>0.030907</td>\n",
       "      <td>0.036511</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/model/CartPole-v1_175710_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 175710}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.00426373, 0.005035245, 0.011928752, 0.01813...</td>\n",
       "      <td>0.030089</td>\n",
       "      <td>0.046499</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/model/CartPole-v1_175710_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 175710}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.00426373, 0.005035245, 0.011928752, 0.01813...</td>\n",
       "      <td>0.030089</td>\n",
       "      <td>0.046499</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/model/CartPole-v1_867808_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 867808}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.00983197, 0.021463675, 0.003928835, 0.00234...</td>\n",
       "      <td>0.032096</td>\n",
       "      <td>0.043715</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>data/model/CartPole-v1_645410_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 645410}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.00797308, 0.00990515, 0.011829001, 0.001321...</td>\n",
       "      <td>0.027737</td>\n",
       "      <td>0.046789</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>data/model/CartPole-v1_903298_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 903298}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.0066479575, 0.0049425503, 0.0062959976, 0.0...</td>\n",
       "      <td>0.031442</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>data/model/CartPole-v1_903298_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 903298}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.0065211104, 0.004849773, 0.0061692004, 0.00...</td>\n",
       "      <td>0.030542</td>\n",
       "      <td>0.039588</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>data/model/CartPole-v1_670967_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 670967}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.0017160082, 0.0027430959, 0.0051479004, 0.0...</td>\n",
       "      <td>0.029649</td>\n",
       "      <td>0.046184</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>data/model/CartPole-v1_670967_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 670967}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[-1.5862597e-05, 0.0010131652, 0.0034225131, 0...</td>\n",
       "      <td>0.029076</td>\n",
       "      <td>0.047039</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   path          env  \\\n",
       "0    data/model/CartPole-v1_56776_1.pth  CartPole-v1   \n",
       "1    data/model/CartPole-v1_56776_2.pth  CartPole-v1   \n",
       "2   data/model/CartPole-v1_175710_1.pth  CartPole-v1   \n",
       "3   data/model/CartPole-v1_175710_2.pth  CartPole-v1   \n",
       "4   data/model/CartPole-v1_867808_1.pth  CartPole-v1   \n",
       "..                                  ...          ...   \n",
       "95  data/model/CartPole-v1_645410_2.pth  CartPole-v1   \n",
       "96  data/model/CartPole-v1_903298_1.pth  CartPole-v1   \n",
       "97  data/model/CartPole-v1_903298_2.pth  CartPole-v1   \n",
       "98  data/model/CartPole-v1_670967_1.pth  CartPole-v1   \n",
       "99  data/model/CartPole-v1_670967_2.pth  CartPole-v1   \n",
       "\n",
       "                              llm         llm_param algo  \\\n",
       "0   qwen2.5-coder_llama3.2-vision   {'seed': 56776}  PPO   \n",
       "1   qwen2.5-coder_llama3.2-vision   {'seed': 56776}  PPO   \n",
       "2   qwen2.5-coder_llama3.2-vision  {'seed': 175710}  PPO   \n",
       "3   qwen2.5-coder_llama3.2-vision  {'seed': 175710}  PPO   \n",
       "4   qwen2.5-coder_llama3.2-vision  {'seed': 867808}  PPO   \n",
       "..                            ...               ...  ...   \n",
       "95  qwen2.5-coder_llama3.2-vision  {'seed': 645410}  PPO   \n",
       "96  qwen2.5-coder_llama3.2-vision  {'seed': 903298}  PPO   \n",
       "97  qwen2.5-coder_llama3.2-vision  {'seed': 903298}  PPO   \n",
       "98  qwen2.5-coder_llama3.2-vision  {'seed': 670967}  PPO   \n",
       "99  qwen2.5-coder_llama3.2-vision  {'seed': 670967}  PPO   \n",
       "\n",
       "                                           algo_param  total_timesteps  \\\n",
       "0   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "1   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "2   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "3   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "4   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "..                                                ...              ...   \n",
       "95  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "96  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "97  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "98  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "99  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "\n",
       "                                      reward_function  \\\n",
       "0   def reward_func(observations:np.ndarray, is_su...   \n",
       "1   def reward_func(observations:np.ndarray, is_su...   \n",
       "2   def reward_func(observations:np.ndarray, is_su...   \n",
       "3   def reward_func(observations:np.ndarray, is_su...   \n",
       "4   def reward_func(observations:np.ndarray, is_su...   \n",
       "..                                                ...   \n",
       "95  def reward_func(observations:np.ndarray, is_su...   \n",
       "96  def reward_func(observations:np.ndarray, is_su...   \n",
       "97  def reward_func(observations:np.ndarray, is_su...   \n",
       "98  def reward_func(observations:np.ndarray, is_su...   \n",
       "99  def reward_func(observations:np.ndarray, is_su...   \n",
       "\n",
       "                                              rewards  mean_reward  \\\n",
       "0   [0.0013466388, 0.006040589, 0.0059295106, 0.00...     0.030907   \n",
       "1   [0.0013466388, 0.006040589, 0.0059295106, 0.00...     0.030907   \n",
       "2   [0.00426373, 0.005035245, 0.011928752, 0.01813...     0.030089   \n",
       "3   [0.00426373, 0.005035245, 0.011928752, 0.01813...     0.030089   \n",
       "4   [0.00983197, 0.021463675, 0.003928835, 0.00234...     0.032096   \n",
       "..                                                ...          ...   \n",
       "95  [0.00797308, 0.00990515, 0.011829001, 0.001321...     0.027737   \n",
       "96  [0.0066479575, 0.0049425503, 0.0062959976, 0.0...     0.031442   \n",
       "97  [0.0065211104, 0.004849773, 0.0061692004, 0.00...     0.030542   \n",
       "98  [0.0017160082, 0.0027430959, 0.0051479004, 0.0...     0.029649   \n",
       "99  [-1.5862597e-05, 0.0010131652, 0.0034225131, 0...     0.029076   \n",
       "\n",
       "    std_reward    sr  \n",
       "0     0.036511  0.73  \n",
       "1     0.036511  0.73  \n",
       "2     0.046499  1.00  \n",
       "3     0.046499  1.00  \n",
       "4     0.043715  0.09  \n",
       "..         ...   ...  \n",
       "95    0.046789  1.00  \n",
       "96    0.039200  0.07  \n",
       "97    0.039588  0.18  \n",
       "98    0.046184  0.97  \n",
       "99    0.047039  0.99  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_img = pd.read_csv('CartPole_v1_log_img.csv', delimiter=';')\n",
    "data_img['rewards'] = data_img['rewards'].map(lambda x : np.array(list(map(float, x.split(',')))))\n",
    "#data_img = data_img.where(data_img['sr']>0.90).dropna()\n",
    "data_img.where(data_img['llm'] == 'qwen2.5-coder_llama3.2-vision')\n",
    "data_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data std rw count    100.000000\n",
      "mean       0.035848\n",
      "std        0.016797\n",
      "min        0.003631\n",
      "25%        0.023814\n",
      "50%        0.044987\n",
      "75%        0.047723\n",
      "max        0.052529\n",
      "Name: std_reward, dtype: float64\n",
      "data_img std rw count    100.000000\n",
      "mean       0.036395\n",
      "std        0.014872\n",
      "min        0.001545\n",
      "25%        0.032525\n",
      "50%        0.042912\n",
      "75%        0.047072\n",
      "max        0.051213\n",
      "Name: std_reward, dtype: float64\n",
      "\n",
      "data mean rw count    100.000000\n",
      "mean       0.009416\n",
      "std        0.027375\n",
      "min       -0.050474\n",
      "25%       -0.017714\n",
      "50%        0.027657\n",
      "75%        0.030107\n",
      "max        0.034805\n",
      "Name: mean_reward, dtype: float64\n",
      "data_img mean rw count    100.000000\n",
      "mean       0.013018\n",
      "std        0.024692\n",
      "min       -0.048952\n",
      "25%       -0.014831\n",
      "50%        0.027390\n",
      "75%        0.030935\n",
      "max        0.034601\n",
      "Name: mean_reward, dtype: float64\n",
      "\n",
      "data sr count    100.000000\n",
      "mean       0.519600\n",
      "std        0.459842\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.625000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: sr, dtype: float64\n",
      "data_img sr count    100.000000\n",
      "mean       0.433200\n",
      "std        0.432344\n",
      "min        0.000000\n",
      "25%        0.007500\n",
      "50%        0.235000\n",
      "75%        0.990000\n",
      "max        1.000000\n",
      "Name: sr, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('data std rw',data['std_reward'].describe())\n",
    "print('data_img std rw',data_img['std_reward'].describe())\n",
    "print()\n",
    "\n",
    "print('data mean rw',data['mean_reward'].describe())\n",
    "print('data_img mean rw',data_img['mean_reward'].describe())\n",
    "print()\n",
    "\n",
    "print('data sr',data['sr'].describe())\n",
    "print('data_img sr',data_img['sr'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('log/LunarLander_v3_log.csv', delimiter=';')\n",
    "data['rewards'] = data['rewards'].map(lambda x : np.array(list(map(float, x.split(',')))))\n",
    "data = data.where(data['SR']>0.2).dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.loc[37].reward_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    \"\"\"Reward function for Hopper-v5\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    height = observations[0]\n",
      "    velocity_x = observations[5]\n",
      "    velocity_z = observations[6]\n",
      "\n",
      "    reward = 0.0\n",
      "\n",
      "    if is_success:\n",
      "        reward += 100.0\n",
      "    elif is_failure:\n",
      "        reward -= 100.0\n",
      "\n",
      "    if height > 1.0 and velocity_x > 0.5 and velocity_z > -0.2:\n",
      "        reward += height * velocity_x + velocity_z\n",
      "\n",
      "    return reward\n",
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    \"\"\"Reward function for Hopper-v5\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    if is_success:\n",
      "        return 10.0\n",
      "    elif is_failure:\n",
      "        return -5.0\n",
      "    else:\n",
      "        x_position = observations[6]\n",
      "        y_position = observations[7]\n",
      "        z_velocity = observations[2]\n",
      "\n",
      "        distance_to_center = np.sqrt(x_position**2 + y_position**2)\n",
      "        velocity_penalty = -abs(z_velocity) * 0.1\n",
      "\n",
      "        return -distance_to_center + velocity_penalty\n",
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    \"\"\"Reward function for Hopper-v5\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    z_position = observations[0]\n",
      "    torso_angle = observations[1]\n",
      "\n",
      "    # Reward for keeping the torso at the correct height and orientation\n",
      "    height_reward = np.clip(1.0 - abs(z_position - 1.0), 0, None)\n",
      "    orientation_reward = np.clip(np.cos(torso_angle) + 1, 0, None)\n",
      "\n",
      "    # Penalty for falling or landing too hard\n",
      "    fall_penalty = -np.maximum(0, z_position)\n",
      "\n",
      "    total_reward = height_reward * orientation_reward + fall_penalty\n",
      "\n",
      "    if is_success:\n",
      "        total_reward += 5.0  # Large reward for successful backflip\n",
      "\n",
      "    if is_failure:\n",
      "        total_reward -= 2.0  # Penalty for failure\n",
      "\n",
      "    return total_reward\n",
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    \"\"\"Reward function for Hopper-v5\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    z_position = observations[0]\n",
      "    torso_angle = observations[1]\n",
      "    thigh_angle = observations[2]\n",
      "    leg_angle = observations[3]\n",
      "    foot_angle = observations[4]\n",
      "\n",
      "    # Reward for maintaining balance and a proper angle during the flip\n",
      "    balance_reward = -np.abs(torso_angle) ** 2\n",
      "\n",
      "    # Reward for height, encouraging the agent to jump higher\n",
      "    height_reward = z_position * 0.1\n",
      "\n",
      "    # Penalize failure\n",
      "    if is_failure:\n",
      "        return -10.0\n",
      "\n",
      "    # Total reward\n",
      "    total_reward = balance_reward + height_reward\n",
      "\n",
      "    return total_reward\n",
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    \"\"\"Reward function for Hopper-v5\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    height = observations[0]\n",
      "    torso_angle = observations[1]\n",
      "    thigh_velocity = observations[8]\n",
      "\n",
      "    # Reward for being above a certain height\n",
      "    height_reward = max(0, height - 0.5)\n",
      "\n",
      "    # Penalize large torso angles\n",
      "    angle_penalty = abs(torso_angle) * -0.1\n",
      "\n",
      "    # Reward for generating force with the thigh joint\n",
      "    velocity_reward = min(1, thigh_velocity) * 0.2\n",
      "\n",
      "    total_reward = height_reward + angle_penalty + velocity_reward\n",
      "\n",
      "    if is_failure:\n",
      "        total_reward -= 10\n",
      "\n",
      "    return total_reward\n",
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    height = observations[0]\n",
      "    torso_angle = observations[1]\n",
      "    thigh_angle = observations[2]\n",
      "    leg_angle = observations[3]\n",
      "    foot_angle = observations[4]\n",
      "    x_velocity = observations[5]\n",
      "    z_velocity = observations[6]\n",
      "\n",
      "    # Reward for maintaining a balance\n",
      "    balance_reward = 0.5 * np.cos(torso_angle) + 0.3 * np.cos(thigh_angle) + 0.2 * np.cos(leg_angle)\n",
      "\n",
      "    # Reward for vertical motion towards the backflip initiation height\n",
      "    target_height = 1.0  # Example target height for initiating a backflip\n",
      "    height_reward = max(0, target_height - abs(height)) / target_height\n",
      "\n",
      "    # Penalize horizontal movement during initial phase\n",
      "    horizontal_penalty = -0.1 * abs(x_velocity)\n",
      "\n",
      "    # Reward for forward motion once the backflip is initiated\n",
      "    forward_motion_reward = 0.05 * z_velocity if height > target_height else 0\n",
      "\n",
      "    total_reward = balance_reward + height_reward + horizontal_penalty + forward_motion_reward\n",
      "\n",
      "    return total_reward\n",
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    \"\"\"Reward function for Hopper-v5\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    z_position = observations[0]\n",
      "    torso_angle = observations[1]\n",
      "    thigh_angle = observations[2]\n",
      "    leg_angle = observations[3]\n",
      "    foot_angle = observations[4]\n",
      "\n",
      "    # Reward for height achieved\n",
      "    height_reward = max(0, z_position)\n",
      "\n",
      "    # Penalty for being too flat or upside down\n",
      "    angle_penalty = -np.abs(torso_angle) * 10\n",
      "\n",
      "    # Reward for leg and foot angles during the kick-off and landing phases\n",
      "    leg_foot_reward = np.cos(thigh_angle) + np.cos(leg_angle) + np.cos(foot_angle)\n",
      "\n",
      "    # Combine rewards\n",
      "    total_reward = height_reward + angle_penalty + leg_foot_reward\n",
      "\n",
      "    return total_reward\n",
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    \"\"\"Reward function for Hopper-v5\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    \n",
      "    height = observations[0]\n",
      "    torso_angle = observations[1]\n",
      "    thigh_angle = observations[2]\n",
      "    leg_angle = observations[3]\n",
      "    foot_angle = observations[4]\n",
      "    x_velocity = observations[5]\n",
      "    z_velocity = observations[6]\n",
      "    torso_angular_velocity = observations[7]\n",
      "    thigh_angular_velocity = observations[8]\n",
      "    leg_angular_velocity = observations[9]\n",
      "    foot_angular_velocity = observations[10]\n",
      "\n",
      "    # Reward for maintaining height above a certain threshold\n",
      "    height_threshold = 1.0\n",
      "    height_reward = max(0, (height - height_threshold) * 10)\n",
      "\n",
      "    # Penalize large torso angle deviation from vertical position\n",
      "    torso_angle_penalty = abs(torso_angle) * 1.0\n",
      "\n",
      "    # Reward for forward movement\n",
      "    forward_movement_reward = x_velocity * 0.5\n",
      "\n",
      "    # Penalty for excessive joint angular velocities\n",
      "    joint_angular_velocity_penalty = (abs(thigh_angular_velocity) +\n",
      "                                     abs(leg_angular_velocity) +\n",
      "                                     abs(foot_angular_velocity)) * 0.1\n",
      "\n",
      "    # Total reward\n",
      "    total_reward = height_reward - torso_angle_penalty + forward_movement_reward - joint_angular_velocity_penalty\n",
      "\n",
      "    return total_reward\n",
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    \"\"\"Reward function for Hopper-v5\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    z_position = observations[0]\n",
      "    x_velocity = observations[5]\n",
      "\n",
      "    # Reward for height\n",
      "    height_reward = max(0, z_position - 1.0) * 10.0\n",
      "\n",
      "    # Penalize forward movement during backflip\n",
      "    forward_penalty = np.abs(x_velocity) * -2.0\n",
      "\n",
      "    # Penalize falling too far\n",
      "    fall_penalty = min(0, z_position - 2.0) * -5.0\n",
      "\n",
      "    total_reward = height_reward + forward_penalty + fall_penalty\n",
      "\n",
      "    return total_reward\n",
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    \"\"\"Reward function for Hopper-v5\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    height = observations[0]\n",
      "    torso_velocity_z = observations[6]\n",
      "    thigh_angle = observations[2]\n",
      "\n",
      "    # Reward for being above a certain height threshold\n",
      "    height_reward = max(height - 1.0, 0)\n",
      "\n",
      "    # Penalize falling down (negative velocity in z-direction)\n",
      "    fall_penalty = max(-torso_velocity_z, 0) * 5\n",
      "\n",
      "    # Reward for moving the thigh joint towards a backflip position\n",
      "    backflip_thigh_angle_target = np.pi  # Target angle for backflip\n",
      "    backflip_reward = 1 - abs(thigh_angle - backflip_thigh_angle_target)\n",
      "\n",
      "    total_reward = height_reward + fall_penalty + backflip_reward\n",
      "\n",
      "    return total_reward\n",
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    \"\"\"Reward function for Hopper-v5\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    z_position = observations[0]\n",
      "    velocity_z = observations[6]\n",
      "\n",
      "    # Reward for height from which the hopper starts the backflip\n",
      "    height_reward = max(0, 1 - abs(z_position))\n",
      "\n",
      "    # Reward for upward movement (positive vertical velocity)\n",
      "    up_velocity_reward = max(0, velocity_z)\n",
      "\n",
      "    # Penalize failure\n",
      "    failure_penalty = -10 if is_failure else 0\n",
      "\n",
      "    return height_reward + up_velocity_reward + failure_penalty\n",
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    \"\"\"Reward function for Hopper-v5\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    height = observations[0]\n",
      "    vertical_velocity = observations[6]\n",
      "    torso_angle = abs(observations[1])\n",
      "    \n",
      "    reward = 0.0\n",
      "    \n",
      "    # Penalize being too close to failure states\n",
      "    if is_failure:\n",
      "        reward -= 10.0\n",
      "    \n",
      "    # Reward for staying upright (reducing torso angle)\n",
      "    reward -= torso_angle * 0.1\n",
      "    \n",
      "    # Reward for upward movement and height\n",
      "    if vertical_velocity > 0:\n",
      "        reward += vertical_velocity * 0.5\n",
      "    reward += max(height - 0.8, 0) * 2.0  # Encourage reaching a certain height\n",
      "    \n",
      "    return reward\n",
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    height = observations[0]\n",
      "    torso_angle = observations[1]\n",
      "    leg_angle = observations[3]\n",
      "    foot_angle = observations[4]\n",
      "    z_velocity = observations[6]\n",
      "    angular_torso_velocity = observations[7]\n",
      "    \n",
      "    reward = 0.0\n",
      "    \n",
      "    # Reward for maintaining a certain height\n",
      "    reward += 2 * (height - 1.3)\n",
      "    \n",
      "    # Penalize large torso angle deviations from upright position\n",
      "    reward -= 5 * abs(torso_angle)\n",
      "    \n",
      "    # Reward for proper leg positioning during backflip\n",
      "    reward -= 0.5 * abs(leg_angle - np.pi)\n",
      "    \n",
      "    # Penalize foot angle deviation during the flip\n",
      "    reward -= 0.3 * abs(foot_angle)\n",
      "    \n",
      "    # Penalize large z-velocity to encourage controlled landings\n",
      "    reward -= 0.2 * abs(z_velocity)\n",
      "    \n",
      "    # Penalize high angular torso velocity for stability\n",
      "    reward -= 0.1 * abs(angular_torso_velocity)\n",
      "    \n",
      "    return reward\n",
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    z_position = observations[0]\n",
      "    torso_angle = observations[1]\n",
      "    x_velocity = observations[5]\n",
      "    angular_velocities = observations[7:11]\n",
      "\n",
      "    # Reward for height increase\n",
      "    height_reward = max(0, z_position - 0.7)\n",
      "\n",
      "    # Reward for torso angle (close to pi for backflip)\n",
      "    angle_reward = -abs(torso_angle - np.pi)\n",
      "\n",
      "    # Reward for x_velocity (to encourage forward motion)\n",
      "    velocity_reward = max(0, x_velocity)\n",
      "\n",
      "    # Penalty for high angular velocities\n",
      "    angular_penalty = -np.sum(np.abs(angular_velocities))\n",
      "\n",
      "    reward = height_reward + angle_reward + velocity_reward + 0.1 * angular_penalty\n",
      "\n",
      "    return reward\n",
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    z_position = observations[0]\n",
      "    torso_angle = observations[1]\n",
      "    thigh_angle = observations[2]\n",
      "    leg_angle = observations[3]\n",
      "    foot_angle = observations[4]\n",
      "    torso_z_velocity = observations[6]\n",
      "\n",
      "    height_reward = np.clip(z_position, 0.75, 1.5) - 0.75\n",
      "    balance_reward = -np.abs(torso_angle)\n",
      "    joint_control_reward = -0.01 * (np.abs(thigh_angle) + np.abs(leg_angle) + np.abs(foot_angle))\n",
      "    velocity_reward = torso_z_velocity if torso_z_velocity > 0 else 0\n",
      "\n",
      "    reward = height_reward + balance_reward + joint_control_reward + velocity_reward\n",
      "    return reward\n",
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    height = observations[0]\n",
      "    torso_angle = observations[1]\n",
      "    thigh_joint_angle = observations[2]\n",
      "    leg_joint_angle = observations[3]\n",
      "    foot_joint_angle = observations[4]\n",
      "    z_velocity = observations[6]\n",
      "\n",
      "    reward = 0.0\n",
      "\n",
      "    # Reward for reaching a certain height\n",
      "    if height > 1.5:\n",
      "        reward += (height - 1.5) * 2.0\n",
      "\n",
      "    # Penalty for deviating from upright position\n",
      "    reward -= abs(torso_angle) * 0.5\n",
      "\n",
      "    # Reward for preparing the jump with thigh joint angle\n",
      "    if thigh_joint_angle > 0.7:\n",
      "        reward += (thigh_joint_angle - 0.7) * 1.0\n",
      "\n",
      "    # Reward for controlling the flip with leg joint angle\n",
      "    if leg_joint_angle < -1.5:\n",
      "        reward += (-1.5 - leg_joint_angle) * 1.0\n",
      "\n",
      "    # Penalty for incorrect foot joint angle during landing\n",
      "    if foot_joint_angle > 0.2:\n",
      "        reward -= abs(foot_joint_angle) * 0.8\n",
      "\n",
      "    # Reward for maintaining upward velocity during the flip\n",
      "    if z_velocity > 3.0:\n",
      "        reward += (z_velocity - 3.0) * 1.5\n",
      "\n",
      "    return reward\n",
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    z_coord = observations[0]\n",
      "    angle_torso = observations[1]\n",
      "    ang_vel_torso = observations[7]\n",
      "    \n",
      "    height_reward = np.clip(z_coord, 0.5, 2.0) - 0.5\n",
      "    rotation_reward = np.cos(angle_torso)\n",
      "    angular_velocity_penalty = -np.abs(ang_vel_torso)\n",
      "    \n",
      "    reward = height_reward + rotation_reward + angular_velocity_penalty\n",
      "    \n",
      "    return reward\n",
      "def reward_func(observations: np.ndarray, is_success: bool, is_failure: bool) -> float:\n",
      "    height = observations[0]\n",
      "    torso_angle = observations[1]\n",
      "    z_velocity = observations[6]\n",
      "    \n",
      "    reward = 0.0\n",
      "    \n",
      "    if height > 0.8:\n",
      "        reward += height * 2\n",
      "    else:\n",
      "        reward -= (0.8 - height) ** 2\n",
      "    \n",
      "    reward -= abs(torso_angle) * 0.5\n",
      "    \n",
      "    if z_velocity > 0:\n",
      "        reward += z_velocity\n",
      "    \n",
      "    return reward\n",
      "def reward_func(observations:np.ndarray, is_success:bool, is_failure:bool) -> float:\n",
      "    \"\"\"Reward function for Hopper-v5\n",
      "\n",
      "    Args:\n",
      "        observations (np.ndarray): observation on the current state\n",
      "        is_success (bool): True if the goal is achieved, False otherwise\n",
      "        is_failure (bool): True if the episode ends unsuccessfully, False otherwise\n",
      "\n",
      "    Returns:\n",
      "        float: The reward for the current step\n",
      "    \"\"\"\n",
      "    z_position = observations[0]\n",
      "    torso_angle = observations[1]\n",
      "    thigh_angle = observations[2]\n",
      "    leg_angle = observations[3]\n",
      "    foot_angle = observations[4]\n",
      "    x_velocity = observations[5]\n",
      "    z_velocity = observations[6]\n",
      "    angular_torso_vel = observations[7]\n",
      "    angular_thigh_vel = observations[8]\n",
      "    angular_leg_vel = observations[9]\n",
      "    angular_foot_vel = observations[10]\n",
      "\n",
      "    reward = 0.0\n",
      "\n",
      "    # Penalize failure\n",
      "    if is_failure:\n",
      "        return -20.0\n",
      "\n",
      "    # Reward for being in the air and close to the peak of the backflip\n",
      "    if z_position > 0.5:\n",
      "        reward += 2 * (z_position ** 1.5)\n",
      "\n",
      "    # Penalty for excessive horizontal movement\n",
      "    reward -= np.abs(x_velocity) * 0.1\n",
      "\n",
      "    # Reward for appropriate angular velocities during the flip\n",
      "    if np.abs(torso_angle) < 0.5 and np.abs(thigh_angle) > 1.0:\n",
      "        reward += 0.2 * (angular_torso_vel ** 2 + angular_thigh_vel ** 2)\n",
      "\n",
      "    return reward\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Hopper_v5_log.csv', delimiter=';')\n",
    "data['rewards'] = data['rewards'].map(lambda x : np.array(list(map(float, x.split(',')))))\n",
    "\n",
    "for func in data['reward_function']:\n",
    "\tprint(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.iloc[1].reward_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, rew in enumerate(data.iloc['rewards']):\n",
    "\tplt.plot(rew, label=idx, alpha=0.7)\n",
    "\tplt.legend()\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
