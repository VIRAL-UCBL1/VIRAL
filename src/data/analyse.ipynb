{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward(data: pd.DataFrame) -> None:\n",
    "\tfor index, row in data[['rewards', 'sr']].iterrows():\n",
    "\t\trew = row['rewards']\n",
    "\t\tsr = row['sr']\n",
    "\t\tplt.plot(rew, alpha=0.7)\n",
    "\tplt.legend()\n",
    "\tplt.ylabel('normalized cumulative reward')\n",
    "\tplt.xlabel('episodes')\n",
    "\tplt.title('CartPole')\n",
    "\tplt.suptitle('Comparison of learning converge speeds')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>env</th>\n",
       "      <th>llm</th>\n",
       "      <th>llm_param</th>\n",
       "      <th>algo</th>\n",
       "      <th>algo_param</th>\n",
       "      <th>total_timesteps</th>\n",
       "      <th>reward_function</th>\n",
       "      <th>rewards</th>\n",
       "      <th>mean_reward</th>\n",
       "      <th>std_reward</th>\n",
       "      <th>sr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/model/CartPole-v1_56776_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 56776}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.0013466388, 0.006040589, 0.0059295106, 0.00...</td>\n",
       "      <td>0.030907</td>\n",
       "      <td>0.036511</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/model/CartPole-v1_56776_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 56776}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.0013466388, 0.006040589, 0.0059295106, 0.00...</td>\n",
       "      <td>0.030907</td>\n",
       "      <td>0.036511</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/model/CartPole-v1_175710_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 175710}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.00426373, 0.005035245, 0.011928752, 0.01813...</td>\n",
       "      <td>0.030089</td>\n",
       "      <td>0.046499</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/model/CartPole-v1_175710_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 175710}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.00426373, 0.005035245, 0.011928752, 0.01813...</td>\n",
       "      <td>0.030089</td>\n",
       "      <td>0.046499</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/model/CartPole-v1_867808_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 867808}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.00983197, 0.021463675, 0.003928835, 0.00234...</td>\n",
       "      <td>0.032096</td>\n",
       "      <td>0.043715</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>data/model/CartPole-v1_365666_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 365666}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.006858737, 0.00078279676, 0.0024521227, 0.0...</td>\n",
       "      <td>0.031481</td>\n",
       "      <td>0.047054</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>data/model/CartPole-v1_610988_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 610988}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.0030825841, 0.008896213, 0.0022266477, 0.00...</td>\n",
       "      <td>0.029146</td>\n",
       "      <td>0.047805</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>data/model/CartPole-v1_610988_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 610988}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.0034331265, 0.009535602, 0.0025809805, 0.00...</td>\n",
       "      <td>0.029250</td>\n",
       "      <td>0.047031</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>data/model/CartPole-v1_401884_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 401884}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[-0.016892578, -0.017205993, -0.014144864, -0....</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>0.051902</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>data/model/CartPole-v1_401884_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 401884}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[-0.016892578, -0.017205993, -0.014144864, -0....</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>0.051902</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   path          env  \\\n",
       "0    data/model/CartPole-v1_56776_1.pth  CartPole-v1   \n",
       "1    data/model/CartPole-v1_56776_2.pth  CartPole-v1   \n",
       "2   data/model/CartPole-v1_175710_1.pth  CartPole-v1   \n",
       "3   data/model/CartPole-v1_175710_2.pth  CartPole-v1   \n",
       "4   data/model/CartPole-v1_867808_1.pth  CartPole-v1   \n",
       "..                                  ...          ...   \n",
       "95  data/model/CartPole-v1_365666_2.pth  CartPole-v1   \n",
       "96  data/model/CartPole-v1_610988_1.pth  CartPole-v1   \n",
       "97  data/model/CartPole-v1_610988_2.pth  CartPole-v1   \n",
       "98  data/model/CartPole-v1_401884_1.pth  CartPole-v1   \n",
       "99  data/model/CartPole-v1_401884_2.pth  CartPole-v1   \n",
       "\n",
       "                              llm         llm_param algo  \\\n",
       "0   qwen2.5-coder_llama3.2-vision   {'seed': 56776}  PPO   \n",
       "1   qwen2.5-coder_llama3.2-vision   {'seed': 56776}  PPO   \n",
       "2   qwen2.5-coder_llama3.2-vision  {'seed': 175710}  PPO   \n",
       "3   qwen2.5-coder_llama3.2-vision  {'seed': 175710}  PPO   \n",
       "4   qwen2.5-coder_llama3.2-vision  {'seed': 867808}  PPO   \n",
       "..                            ...               ...  ...   \n",
       "95  qwen2.5-coder_llama3.2-vision  {'seed': 365666}  PPO   \n",
       "96  qwen2.5-coder_llama3.2-vision  {'seed': 610988}  PPO   \n",
       "97  qwen2.5-coder_llama3.2-vision  {'seed': 610988}  PPO   \n",
       "98  qwen2.5-coder_llama3.2-vision  {'seed': 401884}  PPO   \n",
       "99  qwen2.5-coder_llama3.2-vision  {'seed': 401884}  PPO   \n",
       "\n",
       "                                           algo_param  total_timesteps  \\\n",
       "0   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "1   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "2   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "3   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "4   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "..                                                ...              ...   \n",
       "95  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "96  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "97  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "98  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "99  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "\n",
       "                                      reward_function  \\\n",
       "0   def reward_func(observations:np.ndarray, is_su...   \n",
       "1   def reward_func(observations:np.ndarray, is_su...   \n",
       "2   def reward_func(observations:np.ndarray, is_su...   \n",
       "3   def reward_func(observations:np.ndarray, is_su...   \n",
       "4   def reward_func(observations:np.ndarray, is_su...   \n",
       "..                                                ...   \n",
       "95  def reward_func(observations:np.ndarray, is_su...   \n",
       "96  def reward_func(observations:np.ndarray, is_su...   \n",
       "97  def reward_func(observations:np.ndarray, is_su...   \n",
       "98  def reward_func(observations:np.ndarray, is_su...   \n",
       "99  def reward_func(observations:np.ndarray, is_su...   \n",
       "\n",
       "                                              rewards  mean_reward  \\\n",
       "0   [0.0013466388, 0.006040589, 0.0059295106, 0.00...     0.030907   \n",
       "1   [0.0013466388, 0.006040589, 0.0059295106, 0.00...     0.030907   \n",
       "2   [0.00426373, 0.005035245, 0.011928752, 0.01813...     0.030089   \n",
       "3   [0.00426373, 0.005035245, 0.011928752, 0.01813...     0.030089   \n",
       "4   [0.00983197, 0.021463675, 0.003928835, 0.00234...     0.032096   \n",
       "..                                                ...          ...   \n",
       "95  [0.006858737, 0.00078279676, 0.0024521227, 0.0...     0.031481   \n",
       "96  [0.0030825841, 0.008896213, 0.0022266477, 0.00...     0.029146   \n",
       "97  [0.0034331265, 0.009535602, 0.0025809805, 0.00...     0.029250   \n",
       "98  [-0.016892578, -0.017205993, -0.014144864, -0....     0.012780   \n",
       "99  [-0.016892578, -0.017205993, -0.014144864, -0....     0.012780   \n",
       "\n",
       "    std_reward    sr  \n",
       "0     0.036511  0.73  \n",
       "1     0.036511  0.73  \n",
       "2     0.046499  1.00  \n",
       "3     0.046499  1.00  \n",
       "4     0.043715  0.09  \n",
       "..         ...   ...  \n",
       "95    0.047054  1.00  \n",
       "96    0.047805  1.00  \n",
       "97    0.047031  1.00  \n",
       "98    0.051902  0.04  \n",
       "99    0.051902  0.05  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('CartPole_v1_log.csv', delimiter=';')\n",
    "data['rewards'] = data['rewards'].map(lambda x : np.array(list(map(float, x.split(',')))))\n",
    "#data = data.where(data['sr']>0.90).dropna()\n",
    "data.where(data['llm'] == 'qwen2.5-coder_llama3.2-vision')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>env</th>\n",
       "      <th>llm</th>\n",
       "      <th>llm_param</th>\n",
       "      <th>algo</th>\n",
       "      <th>algo_param</th>\n",
       "      <th>total_timesteps</th>\n",
       "      <th>reward_function</th>\n",
       "      <th>rewards</th>\n",
       "      <th>mean_reward</th>\n",
       "      <th>std_reward</th>\n",
       "      <th>sr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/model/CartPole-v1_56776_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 56776}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.0013466388, 0.006040589, 0.0059295106, 0.00...</td>\n",
       "      <td>0.030907</td>\n",
       "      <td>0.036511</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/model/CartPole-v1_56776_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 56776}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.0013466388, 0.006040589, 0.0059295106, 0.00...</td>\n",
       "      <td>0.030907</td>\n",
       "      <td>0.036511</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/model/CartPole-v1_175710_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 175710}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.00426373, 0.005035245, 0.011928752, 0.01813...</td>\n",
       "      <td>0.030089</td>\n",
       "      <td>0.046499</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/model/CartPole-v1_175710_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 175710}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.00426373, 0.005035245, 0.011928752, 0.01813...</td>\n",
       "      <td>0.030089</td>\n",
       "      <td>0.046499</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/model/CartPole-v1_867808_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 867808}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.00983197, 0.021463675, 0.003928835, 0.00234...</td>\n",
       "      <td>0.032096</td>\n",
       "      <td>0.043715</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>data/model/CartPole-v1_645410_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 645410}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.00797308, 0.00990515, 0.011829001, 0.001321...</td>\n",
       "      <td>0.027737</td>\n",
       "      <td>0.046789</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>data/model/CartPole-v1_903298_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 903298}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.0066479575, 0.0049425503, 0.0062959976, 0.0...</td>\n",
       "      <td>0.031442</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>data/model/CartPole-v1_903298_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 903298}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.0065211104, 0.004849773, 0.0061692004, 0.00...</td>\n",
       "      <td>0.030542</td>\n",
       "      <td>0.039588</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>data/model/CartPole-v1_670967_1.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 670967}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[0.0017160082, 0.0027430959, 0.0051479004, 0.0...</td>\n",
       "      <td>0.029649</td>\n",
       "      <td>0.046184</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>data/model/CartPole-v1_670967_2.pth</td>\n",
       "      <td>CartPole-v1</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 670967}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>30000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[-1.5862597e-05, 0.0010131652, 0.0034225131, 0...</td>\n",
       "      <td>0.029076</td>\n",
       "      <td>0.047039</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   path          env  \\\n",
       "0    data/model/CartPole-v1_56776_1.pth  CartPole-v1   \n",
       "1    data/model/CartPole-v1_56776_2.pth  CartPole-v1   \n",
       "2   data/model/CartPole-v1_175710_1.pth  CartPole-v1   \n",
       "3   data/model/CartPole-v1_175710_2.pth  CartPole-v1   \n",
       "4   data/model/CartPole-v1_867808_1.pth  CartPole-v1   \n",
       "..                                  ...          ...   \n",
       "95  data/model/CartPole-v1_645410_2.pth  CartPole-v1   \n",
       "96  data/model/CartPole-v1_903298_1.pth  CartPole-v1   \n",
       "97  data/model/CartPole-v1_903298_2.pth  CartPole-v1   \n",
       "98  data/model/CartPole-v1_670967_1.pth  CartPole-v1   \n",
       "99  data/model/CartPole-v1_670967_2.pth  CartPole-v1   \n",
       "\n",
       "                              llm         llm_param algo  \\\n",
       "0   qwen2.5-coder_llama3.2-vision   {'seed': 56776}  PPO   \n",
       "1   qwen2.5-coder_llama3.2-vision   {'seed': 56776}  PPO   \n",
       "2   qwen2.5-coder_llama3.2-vision  {'seed': 175710}  PPO   \n",
       "3   qwen2.5-coder_llama3.2-vision  {'seed': 175710}  PPO   \n",
       "4   qwen2.5-coder_llama3.2-vision  {'seed': 867808}  PPO   \n",
       "..                            ...               ...  ...   \n",
       "95  qwen2.5-coder_llama3.2-vision  {'seed': 645410}  PPO   \n",
       "96  qwen2.5-coder_llama3.2-vision  {'seed': 903298}  PPO   \n",
       "97  qwen2.5-coder_llama3.2-vision  {'seed': 903298}  PPO   \n",
       "98  qwen2.5-coder_llama3.2-vision  {'seed': 670967}  PPO   \n",
       "99  qwen2.5-coder_llama3.2-vision  {'seed': 670967}  PPO   \n",
       "\n",
       "                                           algo_param  total_timesteps  \\\n",
       "0   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "1   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "2   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "3   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "4   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "..                                                ...              ...   \n",
       "95  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "96  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "97  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "98  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "99  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...            30000   \n",
       "\n",
       "                                      reward_function  \\\n",
       "0   def reward_func(observations:np.ndarray, is_su...   \n",
       "1   def reward_func(observations:np.ndarray, is_su...   \n",
       "2   def reward_func(observations:np.ndarray, is_su...   \n",
       "3   def reward_func(observations:np.ndarray, is_su...   \n",
       "4   def reward_func(observations:np.ndarray, is_su...   \n",
       "..                                                ...   \n",
       "95  def reward_func(observations:np.ndarray, is_su...   \n",
       "96  def reward_func(observations:np.ndarray, is_su...   \n",
       "97  def reward_func(observations:np.ndarray, is_su...   \n",
       "98  def reward_func(observations:np.ndarray, is_su...   \n",
       "99  def reward_func(observations:np.ndarray, is_su...   \n",
       "\n",
       "                                              rewards  mean_reward  \\\n",
       "0   [0.0013466388, 0.006040589, 0.0059295106, 0.00...     0.030907   \n",
       "1   [0.0013466388, 0.006040589, 0.0059295106, 0.00...     0.030907   \n",
       "2   [0.00426373, 0.005035245, 0.011928752, 0.01813...     0.030089   \n",
       "3   [0.00426373, 0.005035245, 0.011928752, 0.01813...     0.030089   \n",
       "4   [0.00983197, 0.021463675, 0.003928835, 0.00234...     0.032096   \n",
       "..                                                ...          ...   \n",
       "95  [0.00797308, 0.00990515, 0.011829001, 0.001321...     0.027737   \n",
       "96  [0.0066479575, 0.0049425503, 0.0062959976, 0.0...     0.031442   \n",
       "97  [0.0065211104, 0.004849773, 0.0061692004, 0.00...     0.030542   \n",
       "98  [0.0017160082, 0.0027430959, 0.0051479004, 0.0...     0.029649   \n",
       "99  [-1.5862597e-05, 0.0010131652, 0.0034225131, 0...     0.029076   \n",
       "\n",
       "    std_reward    sr  \n",
       "0     0.036511  0.73  \n",
       "1     0.036511  0.73  \n",
       "2     0.046499  1.00  \n",
       "3     0.046499  1.00  \n",
       "4     0.043715  0.09  \n",
       "..         ...   ...  \n",
       "95    0.046789  1.00  \n",
       "96    0.039200  0.07  \n",
       "97    0.039588  0.18  \n",
       "98    0.046184  0.97  \n",
       "99    0.047039  0.99  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_img = pd.read_csv('CartPole_v1_log_img.csv', delimiter=';')\n",
    "data_img['rewards'] = data_img['rewards'].map(lambda x : np.array(list(map(float, x.split(',')))))\n",
    "#data_img = data_img.where(data_img['sr']>0.90).dropna()\n",
    "data_img.where(data_img['llm'] == 'qwen2.5-coder_llama3.2-vision')\n",
    "data_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data std rw count    100.000000\n",
      "mean       0.035848\n",
      "std        0.016797\n",
      "min        0.003631\n",
      "25%        0.023814\n",
      "50%        0.044987\n",
      "75%        0.047723\n",
      "max        0.052529\n",
      "Name: std_reward, dtype: float64\n",
      "data_img std rw count    100.000000\n",
      "mean       0.036395\n",
      "std        0.014872\n",
      "min        0.001545\n",
      "25%        0.032525\n",
      "50%        0.042912\n",
      "75%        0.047072\n",
      "max        0.051213\n",
      "Name: std_reward, dtype: float64\n",
      "\n",
      "data mean rw count    100.000000\n",
      "mean       0.009416\n",
      "std        0.027375\n",
      "min       -0.050474\n",
      "25%       -0.017714\n",
      "50%        0.027657\n",
      "75%        0.030107\n",
      "max        0.034805\n",
      "Name: mean_reward, dtype: float64\n",
      "data_img mean rw count    100.000000\n",
      "mean       0.013018\n",
      "std        0.024692\n",
      "min       -0.048952\n",
      "25%       -0.014831\n",
      "50%        0.027390\n",
      "75%        0.030935\n",
      "max        0.034601\n",
      "Name: mean_reward, dtype: float64\n",
      "\n",
      "data sr count    100.000000\n",
      "mean       0.519600\n",
      "std        0.459842\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        0.625000\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "Name: sr, dtype: float64\n",
      "data_img sr count    100.000000\n",
      "mean       0.433200\n",
      "std        0.432344\n",
      "min        0.000000\n",
      "25%        0.007500\n",
      "50%        0.235000\n",
      "75%        0.990000\n",
      "max        1.000000\n",
      "Name: sr, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('data std rw',data['std_reward'].describe())\n",
    "print('data_img std rw',data_img['std_reward'].describe())\n",
    "print()\n",
    "\n",
    "print('data mean rw',data['mean_reward'].describe())\n",
    "print('data_img mean rw',data_img['mean_reward'].describe())\n",
    "print()\n",
    "\n",
    "print('data sr',data['sr'].describe())\n",
    "print('data_img sr',data_img['sr'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('log/LunarLander_v3_log.csv', delimiter=';')\n",
    "data['rewards'] = data['rewards'].map(lambda x : np.array(list(map(float, x.split(',')))))\n",
    "data = data.where(data['SR']>0.2).dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.loc[37].reward_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>env</th>\n",
       "      <th>llm</th>\n",
       "      <th>llm_param</th>\n",
       "      <th>algo</th>\n",
       "      <th>algo_param</th>\n",
       "      <th>total_timesteps</th>\n",
       "      <th>reward_function</th>\n",
       "      <th>rewards</th>\n",
       "      <th>mean_reward</th>\n",
       "      <th>std_reward</th>\n",
       "      <th>sr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/model/Hopper-v5_230390_1.pth</td>\n",
       "      <td>Hopper-v5</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 230390}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>800000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[-0.0032453814, -0.0032453814, -0.0032453814, ...</td>\n",
       "      <td>0.015065</td>\n",
       "      <td>0.024387</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/model/Hopper-v5_889800_1.pth</td>\n",
       "      <td>Hopper-v5</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 889800}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>800000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[-0.007523234, -0.009326311, -0.008184834, -0....</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/model/Hopper-v5_958661_1.pth</td>\n",
       "      <td>Hopper-v5</td>\n",
       "      <td>qwen2.5-coder_llama3.2-vision</td>\n",
       "      <td>{'seed': 958661}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>500000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[9.22524e-05, 0.00058053003, 6.438106e-05, 0.0...</td>\n",
       "      <td>0.020140</td>\n",
       "      <td>0.026960</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/model/Hopper-v5_23153_1.pth</td>\n",
       "      <td>Hopper-v5</td>\n",
       "      <td>qwen2.5-coder:14b_llama3.2-vision</td>\n",
       "      <td>{'seed': 23153}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>500000</td>\n",
       "      <td>def reward_func(observations: np.ndarray, is_s...</td>\n",
       "      <td>[-0.0042291353, -0.004285594, -0.0042950455, -...</td>\n",
       "      <td>0.023919</td>\n",
       "      <td>0.032504</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/model/Hopper-v5_344415_1.pth</td>\n",
       "      <td>Hopper-v5</td>\n",
       "      <td>qwen2.5-coder:14b_llama3.2-vision</td>\n",
       "      <td>{'seed': 344415}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>500000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[-0.00049235235, -0.00038762772, -0.0002358130...</td>\n",
       "      <td>0.027090</td>\n",
       "      <td>0.030315</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/model/Hopper-v5_64679_1.pth</td>\n",
       "      <td>Hopper-v5</td>\n",
       "      <td>qwen2.5-coder:14b_llama3.2-vision</td>\n",
       "      <td>{'seed': 64679}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>500000</td>\n",
       "      <td>def reward_func(observations: np.ndarray, is_s...</td>\n",
       "      <td>[0.0011228712, 0.00068919786, 0.0012005027, 0....</td>\n",
       "      <td>0.029004</td>\n",
       "      <td>0.030474</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data/model/Hopper-v5_551174_1.pth</td>\n",
       "      <td>Hopper-v5</td>\n",
       "      <td>qwen2.5-coder:14b_llama3.2-vision</td>\n",
       "      <td>{'seed': 551174}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>500000</td>\n",
       "      <td>def reward_func(observations: np.ndarray, is_s...</td>\n",
       "      <td>[0.0008730803, 0.0010605501, 0.00043371413, 0....</td>\n",
       "      <td>0.024684</td>\n",
       "      <td>0.029682</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data/model/Hopper-v5_470489_1.pth</td>\n",
       "      <td>Hopper-v5</td>\n",
       "      <td>qwen2.5-coder:14b_llama3.2-vision</td>\n",
       "      <td>{'seed': 470489}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>500000</td>\n",
       "      <td>def reward_func(observations: np.ndarray, is_s...</td>\n",
       "      <td>[0.0007634571, 0.0012839583, 0.0007812549, 0.0...</td>\n",
       "      <td>0.025206</td>\n",
       "      <td>0.029354</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>data/model/Hopper-v5_537708_1.pth</td>\n",
       "      <td>Hopper-v5</td>\n",
       "      <td>qwen2.5-coder:14b_llama3.2-vision</td>\n",
       "      <td>{'seed': 537708}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>500000</td>\n",
       "      <td>def reward_func(observations: np.ndarray, is_s...</td>\n",
       "      <td>[0.0014212602, 0.0012732946, 0.0011850788, 0.0...</td>\n",
       "      <td>0.027702</td>\n",
       "      <td>0.030455</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>data/model/Hopper-v5_978439_1.pth</td>\n",
       "      <td>Hopper-v5</td>\n",
       "      <td>qwen2.5-coder:14b_llama3.2-vision</td>\n",
       "      <td>{'seed': 978439}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>500000</td>\n",
       "      <td>def reward_func(observations: np.ndarray, is_s...</td>\n",
       "      <td>[-0.010173625, 0.00077878154, 0.00029526738, 6...</td>\n",
       "      <td>0.007123</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>data/model/Hopper-v5_976297_1.pth</td>\n",
       "      <td>Hopper-v5</td>\n",
       "      <td>qwen2.5-coder:14b_llama3.2-vision</td>\n",
       "      <td>{'seed': 976297}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>500000</td>\n",
       "      <td>def reward_func(observations:np.ndarray, is_su...</td>\n",
       "      <td>[-0.0013257167, -0.0013257167, -0.0013257167, ...</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.024164</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>data/model/Hopper-v5_944289_1.pth</td>\n",
       "      <td>Hopper-v5</td>\n",
       "      <td>qwen2.5-coder:14b_llama3.2-vision</td>\n",
       "      <td>{'seed': 944289}</td>\n",
       "      <td>PPO</td>\n",
       "      <td>{'policy': 'MlpPolicy', 'verbose': 0, 'device'...</td>\n",
       "      <td>500000</td>\n",
       "      <td>def reward_func(observations: np.ndarray, is_s...</td>\n",
       "      <td>[0.0002739189, 0.0011292353, 0.0010027634, -0....</td>\n",
       "      <td>0.023989</td>\n",
       "      <td>0.029525</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 path        env  \\\n",
       "0   data/model/Hopper-v5_230390_1.pth  Hopper-v5   \n",
       "1   data/model/Hopper-v5_889800_1.pth  Hopper-v5   \n",
       "2   data/model/Hopper-v5_958661_1.pth  Hopper-v5   \n",
       "3    data/model/Hopper-v5_23153_1.pth  Hopper-v5   \n",
       "4   data/model/Hopper-v5_344415_1.pth  Hopper-v5   \n",
       "5    data/model/Hopper-v5_64679_1.pth  Hopper-v5   \n",
       "6   data/model/Hopper-v5_551174_1.pth  Hopper-v5   \n",
       "7   data/model/Hopper-v5_470489_1.pth  Hopper-v5   \n",
       "8   data/model/Hopper-v5_537708_1.pth  Hopper-v5   \n",
       "9   data/model/Hopper-v5_978439_1.pth  Hopper-v5   \n",
       "10  data/model/Hopper-v5_976297_1.pth  Hopper-v5   \n",
       "11  data/model/Hopper-v5_944289_1.pth  Hopper-v5   \n",
       "\n",
       "                                  llm         llm_param algo  \\\n",
       "0       qwen2.5-coder_llama3.2-vision  {'seed': 230390}  PPO   \n",
       "1       qwen2.5-coder_llama3.2-vision  {'seed': 889800}  PPO   \n",
       "2       qwen2.5-coder_llama3.2-vision  {'seed': 958661}  PPO   \n",
       "3   qwen2.5-coder:14b_llama3.2-vision   {'seed': 23153}  PPO   \n",
       "4   qwen2.5-coder:14b_llama3.2-vision  {'seed': 344415}  PPO   \n",
       "5   qwen2.5-coder:14b_llama3.2-vision   {'seed': 64679}  PPO   \n",
       "6   qwen2.5-coder:14b_llama3.2-vision  {'seed': 551174}  PPO   \n",
       "7   qwen2.5-coder:14b_llama3.2-vision  {'seed': 470489}  PPO   \n",
       "8   qwen2.5-coder:14b_llama3.2-vision  {'seed': 537708}  PPO   \n",
       "9   qwen2.5-coder:14b_llama3.2-vision  {'seed': 978439}  PPO   \n",
       "10  qwen2.5-coder:14b_llama3.2-vision  {'seed': 976297}  PPO   \n",
       "11  qwen2.5-coder:14b_llama3.2-vision  {'seed': 944289}  PPO   \n",
       "\n",
       "                                           algo_param  total_timesteps  \\\n",
       "0   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...           800000   \n",
       "1   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...           800000   \n",
       "2   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...           500000   \n",
       "3   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...           500000   \n",
       "4   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...           500000   \n",
       "5   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...           500000   \n",
       "6   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...           500000   \n",
       "7   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...           500000   \n",
       "8   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...           500000   \n",
       "9   {'policy': 'MlpPolicy', 'verbose': 0, 'device'...           500000   \n",
       "10  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...           500000   \n",
       "11  {'policy': 'MlpPolicy', 'verbose': 0, 'device'...           500000   \n",
       "\n",
       "                                      reward_function  \\\n",
       "0   def reward_func(observations:np.ndarray, is_su...   \n",
       "1   def reward_func(observations:np.ndarray, is_su...   \n",
       "2   def reward_func(observations:np.ndarray, is_su...   \n",
       "3   def reward_func(observations: np.ndarray, is_s...   \n",
       "4   def reward_func(observations:np.ndarray, is_su...   \n",
       "5   def reward_func(observations: np.ndarray, is_s...   \n",
       "6   def reward_func(observations: np.ndarray, is_s...   \n",
       "7   def reward_func(observations: np.ndarray, is_s...   \n",
       "8   def reward_func(observations: np.ndarray, is_s...   \n",
       "9   def reward_func(observations: np.ndarray, is_s...   \n",
       "10  def reward_func(observations:np.ndarray, is_su...   \n",
       "11  def reward_func(observations: np.ndarray, is_s...   \n",
       "\n",
       "                                              rewards  mean_reward  \\\n",
       "0   [-0.0032453814, -0.0032453814, -0.0032453814, ...     0.015065   \n",
       "1   [-0.007523234, -0.009326311, -0.008184834, -0....    -0.007589   \n",
       "2   [9.22524e-05, 0.00058053003, 6.438106e-05, 0.0...     0.020140   \n",
       "3   [-0.0042291353, -0.004285594, -0.0042950455, -...     0.023919   \n",
       "4   [-0.00049235235, -0.00038762772, -0.0002358130...     0.027090   \n",
       "5   [0.0011228712, 0.00068919786, 0.0012005027, 0....     0.029004   \n",
       "6   [0.0008730803, 0.0010605501, 0.00043371413, 0....     0.024684   \n",
       "7   [0.0007634571, 0.0012839583, 0.0007812549, 0.0...     0.025206   \n",
       "8   [0.0014212602, 0.0012732946, 0.0011850788, 0.0...     0.027702   \n",
       "9   [-0.010173625, 0.00077878154, 0.00029526738, 6...     0.007123   \n",
       "10  [-0.0013257167, -0.0013257167, -0.0013257167, ...     0.015596   \n",
       "11  [0.0002739189, 0.0011292353, 0.0010027634, -0....     0.023989   \n",
       "\n",
       "    std_reward    sr  \n",
       "0     0.024387  0.55  \n",
       "1     0.000653  0.00  \n",
       "2     0.026960  0.00  \n",
       "3     0.032504  0.00  \n",
       "4     0.030315  0.00  \n",
       "5     0.030474  0.00  \n",
       "6     0.029682  0.00  \n",
       "7     0.029354  0.00  \n",
       "8     0.030455  0.00  \n",
       "9     0.001994  0.00  \n",
       "10    0.024164  0.44  \n",
       "11    0.029525  0.76  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Hopper_v5_log.csv', delimiter=';')\n",
    "data['rewards'] = data['rewards'].map(lambda x : np.array(list(map(float, x.split(',')))))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.iloc[1].reward_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, rew in enumerate(data.iloc['rewards']):\n",
    "\tplt.plot(rew, label=idx, alpha=0.7)\n",
    "\tplt.legend()\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
