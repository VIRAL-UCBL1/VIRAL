llm;reward_function;src;SR
qwen2.5-coder;;[0];0.85
qwen2.5-coder;"def reward_func(observations: np.ndarray, terminated: bool, truncated: bool) -> float:
    """"""Reward function for CartPole-v1

    Args:
        observations (np.ndarray): observation on the current state
        terminated (bool): episode is terminated due a failure
        truncated (bool): episode is truncated due a success

    Returns:
        float: The reward for the current step
    """"""
    x, x_dot, theta, theta_dot = observations
    
    # Penalize large angles and angular velocities
    angle_cost = 0.5 * abs(theta)
    
    # Penalize high cart positions
    position_cost = 0.1 * abs(x) ** 2
    
    # Reward for maintaining stability
    if not terminated and not truncated:
        reward = 1.0 - (angle_cost + position_cost)
    else:
        reward = -1.0
    
    return reward";[2];0.98
qwen2.5-coder;"def reward_func(observations: np.ndarray, terminated: bool, truncated: bool) -> float:
    """"""Reward function for CartPole-v1

    Args:
        observations (np.ndarray): observation on the current state
        terminated (bool): episode is terminated due a failure
        truncated (bool): episode is truncated due a success

    Returns:
        float: The reward for the current step
    """"""
    x, x_dot, theta, theta_dot = observations
    
    # Penalize large angles and angular velocities
    angle_cost = 0.2 * abs(theta)
    
    # Penalize high cart positions
    position_cost = 0.1 * abs(x) ** 2
    
    # Reward for maintaining stability
    if not terminated and not truncated:
        reward = 1.0 - (angle_cost + position_cost)
    else:
        reward = -1.0
    
    return reward";[1];0.99
qwen2.5-coder;"def reward_func(observations: np.ndarray, terminated: bool, truncated: bool) -> float:
    """"""Reward function for CartPole-v1

    Args:
        observations (np.ndarray): observation on the current state
        terminated (bool): episode is terminated due a failure
        truncated (bool): episode is truncated due a success

    Returns:
        float: The reward for the current step
    """"""
    x, x_dot, theta, theta_dot = observations
    
    # Penalize large angles and angular velocities
    angle_cost = 0.5 * abs(theta)
    
    # Penalize high cart positions
    position_cost = 0.1 * abs(x) ** 2
    
    # Reward for maintaining stability
    if not terminated and not truncated:
        reward = 1.0 - (angle_cost + position_cost)
    else:
        reward = -1.0
    
    # Boost reward when the cart is near the center
    center_position_threshold = 0.15
    if abs(x) < center_position_threshold:
        reward += 1.0
    
    return reward";[1, 3];0.79
qwen2.5-coder;"def reward_func(observations: np.ndarray, terminated: bool, truncated: bool) -> float:
    """"""Reward function for CartPole-v1

    Args:
        observations (np.ndarray): observation on the current state
        terminated (bool): episode is terminated due a failure
        truncated (bool): episode is truncated due a success

    Returns:
        float: The reward for the current step
    """"""
    x, x_dot, theta, theta_dot = observations
    
    # Penalize large angles and angular velocities
    angle_cost = 0.2 * abs(theta)
    
    # Penalize high cart positions
    position_cost = 0.1 * abs(x) ** 2
    
    # Reward for maintaining stability
    if not terminated and not truncated:
        reward = 1.0 - (angle_cost + position_cost)
    else:
        reward = -1.0
    
    # Boost reward when the cart is near the center
    center_position_threshold = 0.15
    if abs(x) < center_position_threshold:
        reward += 2.0
    
    return reward";[1, 4];0.0
qwen2.5-coder;"def reward_func(observations: np.ndarray, terminated: bool, truncated: bool) -> float:
    """"""Reward function for CartPole-v1

    Args:
        observations (np.ndarray): observation on the current state
        terminated (bool): episode is terminated due a failure
        truncated (bool): episode is truncated due a success

    Returns:
        float: The reward for the current step
    """"""
    x, x_dot, theta, theta_dot = observations
    
    # Penalize large angles and angular velocities
    angle_cost = 0.2 * abs(theta)
    
    # Penalize high cart positions
    position_cost = 0.1 * abs(x) ** 2
    
    # Reward for maintaining stability
    if not terminated and not truncated:
        reward = 1.0 - (angle_cost + position_cost)
    else:
        reward = -1.0
    
    # Boost reward when the cart is near the center
    center_position_threshold = 0.15
    if abs(x) < center_position_threshold:
        reward += 3.0
    
    return reward";[1, 5];0.0
qwen2.5-coder;;[0];0.89
qwen2.5-coder;"def reward_func(observations: np.ndarray, terminated: bool, truncated: bool) -> float:
    """"""Reward function for CartPole-v1

    Args:
        observations (np.ndarray): observation on the current state
        terminated (bool): episode is terminated due a failure
        truncated (bool): episode is truncated due a success

    Returns:
        float: The reward for the current step
    """"""
    x, _, theta, _ = observations
    if terminated or truncated:
        return -1.0
    else:
        return 1.0 - abs(theta) / np.pi + (0.5 - abs(x) / 2.4)";[2];0.45
qwen2.5-coder;"def reward_func(observations: np.ndarray, terminated: bool, truncated: bool) -> float:
    """"""Reward function for CartPole-v1

    Args:
        observations (np.ndarray): observation on the current state
        terminated (bool): episode is terminated due a failure
        truncated (bool): episode is truncated due a success

    Returns:
        float: The reward for the current step
    """"""
    x, _, theta, _ = observations
    if terminated or truncated:
        return -1.0
    else:
        return 1.0 - abs(theta) / np.pi";[1];1.0
qwen2.5-coder;"def reward_func(observations: np.ndarray, terminated: bool, truncated: bool) -> float:
    """"""Reward function for CartPole-v1

    Args:
        observations (np.ndarray): observation on the current state
        terminated (bool): episode is terminated due a failure
        truncated (bool): episode is truncated due a success

    Returns:
        float: The reward for the current step
    """"""
    x, _, theta, _ = observations
    if terminated or truncated:
        return -1.0
    else:
        x_reward = 1.0 - abs(x) / 2.4
        theta_reward = 1.0 - abs(theta) / np.pi
        return x_reward * theta_reward + 0.5";[1, 3];0.81
qwen2.5-coder;"def reward_func(observations: np.ndarray, terminated: bool, truncated: bool) -> float:
    """"""Reward function for CartPole-v1

    Args:
        observations (np.ndarray): observation on the current state
        terminated (bool): episode is terminated due a failure
        truncated (bool): episode is truncated due a success

    Returns:
        float: The reward for the current step
    """"""
    x, _, theta, _ = observations
    if terminated or truncated:
        return -1.0
    else:
        distance_reward = 1.0 - abs(x) / 2.4
        angle_reward = 1.0 - abs(theta) / np.pi
        balance_bonus = 5.0 * (1.0 - abs(theta) / (np.pi / 8))
        return distance_reward + angle_reward + balance_bonus";[1, 4];0.0
qwen2.5-coder;"def reward_func(observations: np.ndarray, terminated: bool, truncated: bool) -> float:
    """"""Reward function for CartPole-v1

    Args:
        observations (np.ndarray): observation on the current state
        terminated (bool): episode is terminated due a failure
        truncated (bool): episode is truncated due a success

    Returns:
        float: The reward for the current step
    """"""
    x, _, theta, _ = observations
    if terminated or truncated:
        return -1.0
    else:
        distance_reward = 1.0 - abs(x) / 2.4
        angle_reward = 1.0 - abs(theta) / np.pi
        balance_bonus = 5.0 * (1.0 - abs(theta) / (np.pi / 8))
        return distance_reward + angle_reward + balance_bonus + (x**2 * theta**2) / 100.0";[1, 5];0.0
