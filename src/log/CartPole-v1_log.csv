env;llm;reward_function;SR
CartPole-v1;qwen2.5-coder;;0.66
CartPole-v1;qwen2.5-coder;"import numpy as np

def reward_func(observations:np.ndarray, terminated: bool, truncated: bool) -> float:
    """"""Reward function for CartPole-v1

    Args:
        observations (np.ndarray): observation on the current state
        terminated (bool): episode is terminated due a failure
        truncated (bool): episode is truncated due a success

    Returns:
        float: The reward for the current step
    """"""
    x, x_dot, theta, theta_dot = observations
    
    # Penalize large positions and angles
    position_reward = -abs(x) * 0.1
    angle_reward = -abs(theta) * 0.1
    
    # Reward for successful balance
    if not terminated and not truncated:
        reward = 1.0 + position_reward + angle_reward
    else:
        reward = -1.0
    
    return reward";0.82
CartPole-v1;qwen2.5-coder;"import numpy as np

def reward_func(observations:np.ndarray, terminated: bool, truncated: bool) -> float:
    """"""Reward function for CartPole-v1

    Args:
        observations (np.ndarray): observation on the current state
        terminated (bool): episode is terminated due a failure
        truncated (bool): episode is truncated due a success

    Returns:
        float: The reward for the current step
    """"""
    x, x_dot, theta, theta_dot = observations
    
    # Penalize large positions and angles
    position_reward = -abs(x) * 0.1
    angle_reward = -abs(theta) * 0.1
    
    # Reward for successful balance
    if not terminated and not truncated:
        reward = 1.0 + position_reward + angle_reward
    else:
        reward = -1.0
    
    return reward";0.82
CartPole-v1;qwen2.5-coder;"import numpy as np

def reward_func(observations:np.ndarray, terminated: bool, truncated: bool) -> float:
    """"""Reward function for CartPole-v1

    Args:
        observations (np.ndarray): observation on the current state
        terminated (bool): episode is terminated due a failure
        truncated (bool): episode is truncated due a success

    Returns:
        float: The reward for the current step
    """"""
    x, x_dot, theta, theta_dot = observations
    
    # Penalize large positions and angles
    position_reward = -abs(x) * 0.1
    angle_reward = -abs(theta) * 0.1
    
    # Reward for successful balance
    if not terminated and not truncated:
        reward = 2.0 + position_reward + angle_reward
    else:
        reward = -2.0
    
    return reward";0.1
