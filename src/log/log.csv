"environment_type"; "llm"; "reward_function"; "SR"
CartPole-v1;qwen2.5-coder;"import numpy as np

def reward_func(observations:np.ndarray, terminated: bool, truncated: bool) -> float:
    """"""Reward function for CartPole-v1

    Args:
        observations (np.ndarray): observation on the current state
        terminated (bool): episode is terminated due a failure
        truncated (bool): episode is truncated due a success

    Returns:
        float: The reward for the current step
    """"""
    
    if terminated or truncated:
        return -1.0
    
    x, _, theta, _ = observations
    return 1.0 - abs(x) - abs(theta)";0.0
CartPole-v1;qwen2.5-coder;"import numpy as np

def reward_func(observations:np.ndarray, terminated: bool, truncated: bool) -> float:
    """"""Reward function for CartPole-v1

    Args:
        observations (np.ndarray): observation on the current state
        terminated (bool): episode is terminated due a failure
        truncated (bool): episode is truncated due a success

    Returns:
        float: The reward for the current step
    """"""
    
    if terminated or truncated:
        return -1.0
    
    x, _, theta, _ = observations
    return 1.0 - abs(x) * 2 - abs(theta) * 5";0.0
CartPole-v1;qwen2.5-coder;"import numpy as np

def reward_func(observations:np.ndarray, terminated: bool, truncated: bool) -> float:
    """"""Reward function for CartPole-v1

    Args:
        observations (np.ndarray): observation on the current state
        terminated (bool): episode is terminated due a failure
        truncated (bool): episode is truncated due a success

    Returns:
        float: The reward for the current step
    """"""
    
    if terminated or truncated:
        return -1.0
    
    x, _, theta, _ = observations
    return 2.0 - abs(x) * 2 - abs(theta) * 5";0.0
